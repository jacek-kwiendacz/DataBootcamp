{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2 - modelowanie\n",
    "\n",
    "#### kolejne kroki:\n",
    "- pobranie danych do modelowania\n",
    "- przygotowanie zestawów danych train-test w kilku wariantach\n",
    "- modelowanie z zastosowaniem wybranych klasyfikatorów\n",
    "- omówienie i podsumowanie wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pakietów\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Przygotowanie danych do modelowania\n",
    "\n",
    "#### Import danych\n",
    "\n",
    "Dane do modelowania przygotowane zostały w 4 wersjach - wersje te różnią się sposobem uzupełnienia brakujących wartości w zakresie 5 zmiennych:\n",
    "\n",
    "- 'offer_amount', \n",
    "- 'offer_period', \n",
    "- 'interest_rate', \n",
    "- 'fee', \n",
    "- 'offer_monthly_obligation'.\n",
    "\n",
    "Są to następujące warianty:\n",
    "\n",
    "- 1 - zmienne usunięte ze zbioru danych,\n",
    "- 2 - braki danych uzupełnione medianą,\n",
    "- 3 - braki danych uzupełnione przez losowanie z rozkładu normalnego,\n",
    "- 4 - braki danych uzupełnione przez kopiowanie danych z innych rekordów datasetu.\n",
    "\n",
    "Przygotowanie modelu wykonywane będzie dla każdego z wariantów niezależnie.\n",
    "Sposób uzupełnienia wartości brakujacych zostanie oceniony po analizie jakości uzyskanych modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import danych\n",
    "col_names = ['ID', 'gender', 'city', 'income', 'birth_date', 'application_date', 'requested_amount', \n",
    "             'requested_period', 'financial_obligations', 'employer_name', 'account_bank', \n",
    "             'mobile_verification_flag', 'var_5', 'var_1', 'offer_amount', 'offer_period', 'interest_rate', \n",
    "             'fee', 'offer_monthly_obligation', 'filled_form_flag', 'device', 'var_2', 'source', 'var_4', \n",
    "             'disbursed_flag', 'latitude', 'longitude', 'age']\n",
    "\n",
    "datasets = {\n",
    "    1:{'name':'none'},\n",
    "    2:{'name':'median'},\n",
    "    3:{'name':'distribution'},\n",
    "    4:{'name':'observation'}\n",
    "}\n",
    "\n",
    "datasets[2]['data'] = pd.read_csv('dataset_inputation_median.csv', delimiter = ';', engine='python', header = None, names = col_names, \n",
    "                      index_col = 0, skiprows = 1 )\n",
    "datasets[3]['data'] = pd.read_csv('dataset_inputation_distribution.csv', delimiter = ';', engine='python', header = None, names = col_names, \n",
    "                      index_col = 0, skiprows = 1 )\n",
    "datasets[4]['data'] = pd.read_csv('dataset_inputation_random_observation.csv', delimiter = ';', engine='python', header = None, names = col_names, \n",
    "                      index_col = 0, skiprows = 1 )\n",
    "datasets[1]['data'] = datasets[2]['data'].copy()\n",
    "\n",
    "columns_to_drop = ['city', 'birth_date', 'application_date', 'employer_name', 'account_bank']\n",
    "columns_to_rename = {'ID':'id', \n",
    "                     'gender':'cat01', \n",
    "                     'income':'num01', \n",
    "                     'requested_amount':'num02', \n",
    "                     'requested_period':'num03',\n",
    "                     'financial_obligations':'num04',\n",
    "                     'mobile_verification_flag':'cat02',\n",
    "                     'var_5':'cat03',\n",
    "                     'var_1':'cat04',\n",
    "                     'offer_amount':'num05',\n",
    "                     'offer_period':'num06',\n",
    "                     'interest_rate':'num07',\n",
    "                     'fee':'num08',\n",
    "                     'offer_monthly_obligation':'num09',\n",
    "                     'filled_form_flag':'cat05',\n",
    "                     'device':'cat06',\n",
    "                     'var_2':'cat07',\n",
    "                     'source':'cat08',\n",
    "                     'var_4':'cat09',\n",
    "                     'disbursed_flag':'explained',\n",
    "                     'latitude':'num10',\n",
    "                     'longitude':'num11',\n",
    "                     'age':'num12'\n",
    "}\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['data'] = datasets[item]['data'].drop(columns_to_drop, axis=1).rename(columns=columns_to_rename)\n",
    "\n",
    "datasets[1]['data'] = datasets[1]['data'].drop(['num05','num06','num07','num08','num09'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podział train-test\n",
    "\n",
    "Dla każdego wariantu danych dzielę dataset na zbiór trenujacy i testowy wg proporcji 3:1.\n",
    "Klasa pozytywnych obserwacji zmiennej objaśnianej jest mało liczna, dlatego żeby zachować proporcjonalny podział\n",
    "stosuję opcję 'stratifiy'. \n",
    "\n",
    "Mała liczność obserwacji pozytywnych w zmiennej objaśnianej będzie wymagać zastosowania skalowania w procesie modelowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "licznosc_zbioru = 87020\n",
      "liczność obserwacji pozytywnych dla zmiennej objaśnianej = 1273.0\n",
      "udział obserwacji pozytywnych zmiennej objaśnianej w zbiorze = 0.01462882096069869\n",
      "\n",
      "Dataset 1: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n",
      "Dataset 2: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n",
      "Dataset 3: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n",
      "Dataset 4: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n"
     ]
    }
   ],
   "source": [
    "licznosc = datasets[1]['data'].shape[0]\n",
    "licznosc_y = sum(datasets[1]['data']['explained'])\n",
    "licznosc_y_procent = licznosc_y / licznosc\n",
    "print(f\"licznosc_zbioru = {licznosc}\")\n",
    "print(f\"liczność obserwacji pozytywnych dla zmiennej objaśnianej = {licznosc_y}\")\n",
    "print(f\"udział obserwacji pozytywnych zmiennej objaśnianej w zbiorze = {licznosc_y_procent}\\n\")\n",
    "      \n",
    "# Train - test split\n",
    "train_test_split_ratio = 0.25\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['X'] = datasets[item]['data'].drop(['explained'], axis=1)\n",
    "    datasets[item]['y'] = datasets[item]['data']['explained']\n",
    "    datasets[item]['X_train'], datasets[item]['X_test'], datasets[item]['y_train'], datasets[item]['y_test'] = train_test_split(datasets[item]['X'], datasets[item]['y'], test_size=train_test_split_ratio, random_state=42, stratify=datasets[item]['y'])\n",
    "    y_train_share = sum(datasets[item]['y_train'])/datasets[item]['X_train'].shape[0]\n",
    "    y_test_share  = sum(datasets[item]['y_test']) /datasets[item]['X_test'].shape[0]\n",
    "    print(f\"Dataset {item}: train - {y_train_share} 'jedynek', test - {y_test_share} 'jedynek'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podejście do zmiennych kategorycznych\n",
    "\n",
    "Na etapie przygotowania danych zmienne kategoryczne zostały zakodowane wg rosnącego udziału obserwacji pozytywnych w danej klasie. Bardziej adekwatne podejście (szczególnie w celu zastosowania regresji logistycznej) jest zastosowanie współczynników WOE (Weight Of Evidence), czyli skalowanie wg proporcji ln(liczba obserwacji pozytywnych / liczba obserwacji negatywnych). \n",
    "\n",
    "Dla każdego ze zbiorów danych wyznaczam wagi WOE do zmiany kodowania zmiennych kategorycznych. Do wyliczenia wag wykorzystuję wyłącznie obserwacje ze zbioru treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# WOE calculation for categorical\n",
    "categorical_variables = [name for name in datasets[1]['data'].columns if 'cat' in name]\n",
    "\n",
    "for item in datasets:\n",
    "    WOE_dataset = datasets[item]['X_train'][categorical_variables]\n",
    "    WOE_dataset['positive'] = datasets[item]['y_train']\n",
    "    WOE_dataset['negative'] = WOE_dataset.apply(lambda x: 1 - x['positive'], axis=1)\n",
    "    WOE_mapper = pd.DataFrame(columns=['feature_name', 'feature_code', 'WOE'])\n",
    "    for variable in categorical_variables:\n",
    "        variable_data = WOE_dataset[[variable, 'positive', 'negative']]\n",
    "        out = variable_data.groupby([variable]).sum()\n",
    "        out['WOE'] = out.apply(lambda x: log(x['positive'] / x['negative']) if x['positive'] > 0 and x['negative'] > 0 else -12, axis = 1)\n",
    "        out['feature_name'] = variable\n",
    "        out['feature_code'] = out.index\n",
    "        out = out[['feature_name', 'feature_code', 'WOE']].reset_index(drop=True)\n",
    "        WOE_mapper = pd.concat([WOE_mapper, out])\n",
    "    datasets[item]['WOE_mapper'] = WOE_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodowanie stosuję zarówno do zbioru treningowego, jak i testowego. Wariant danych z kodowaniem WOE oznaczam odrębnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with WOE\n",
    "\n",
    "def WOE_mapping(dataset, WOE_mapper):\n",
    "    output = dataset.copy()\n",
    "    for variable in categorical_variables:\n",
    "        mapper = WOE_mapper[WOE_mapper['feature_name'] == variable]\n",
    "        mapper_dict = {}\n",
    "        for category in mapper.index:\n",
    "            mapper_dict[mapper.loc[category]['feature_code']] = mapper.loc[category]['WOE']\n",
    "        output[variable] = output.apply(lambda x: mapper_dict[x[variable]], axis=1)\n",
    "    return output\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['X_train_woe'] = WOE_mapping(datasets[item]['X_train'], datasets[item]['WOE_mapper'])\n",
    "    datasets[item]['X_test_woe'] = WOE_mapping(datasets[item]['X_test'], datasets[item]['WOE_mapper'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standaryzacja\n",
    "\n",
    "Dane numeryczne w zbiorze danych są różnorodnie skalowane - np. wiek klienta to zmienna z przedziału ok. [0, 100], tymczasem kwota dochodu - już sięga milionów (dochód wyrażony jest w rupiach indyjskich). Zniwelowanie różnic w skali umożliwia zastosowanie standaryzacji zmiennych - stosuję do tego celu wbudowaną funkcjonalność StandardScaler(). \n",
    "\n",
    "Standaryzacji poddaję zarówno zbiory ze zwyczajnym kodowaniem zmiennych kategorycznych, jak i z kodowaniem WOE.\n",
    "Zbiory przeskalowane oznaczam odrębnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scaled\n",
    "for item in datasets:\n",
    "    datasets[item]['scaler'] = StandardScaler()\n",
    "    datasets[item]['WOE_scaler'] = StandardScaler()\n",
    "    datasets[item]['scaler'].fit(datasets[item]['X_train'])\n",
    "    datasets[item]['WOE_scaler'].fit(datasets[item]['X_train_woe'])\n",
    "    datasets[item]['X_train_scaled'] = datasets[item]['scaler'].transform(datasets[item]['X_train'])\n",
    "    datasets[item]['X_test_scaled'] = datasets[item]['scaler'].transform(datasets[item]['X_test'])\n",
    "    datasets[item]['X_train_woe_scaled'] = datasets[item]['WOE_scaler'].transform(datasets[item]['X_train_woe'])\n",
    "    datasets[item]['X_test_woe_scaled'] = datasets[item]['WOE_scaler'].transform(datasets[item]['X_test_woe'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redukcja wymiarowości\n",
    "\n",
    "Korelacja części zmiennych datasetu ze zmienną objaśnianą może być na tyle niska, że zmienna nie wniesie znaczącego udziału w jakość modelu. Redukcja wymiarowości umozliwia wyłuskanie najistotniejszych zmiennych do budowy modelu. Ograniczenie ilości danych usprawnia proces modelowania kosztem niewielkiej straty jakości modelu. Ma to szczególne znaczenie dla zbiorów ze sporą ilością zmiennych modelowych.\n",
    "\n",
    "W naszym przypadku redukcja wymiarowości nie jest konieczna z uwagi na niedużą liczbę zmiennych modleowych (poniżej 20). Mimo to wyznaczam zbiór danych z odcięciem części zmiennych - warunkiem jest obniżenie wyjaśnianej wariancji maksymalnie o 5%. Wykorzystuję Principal Component Analysis (jest dostępna wbudowana funkcjonalność w bibliotece sklearn).\n",
    "\n",
    "Procedurze PCA poddaje się zbiory po regularyzacji, wyliczenie stosuję zarówno do zbioru ze zwyczajnym kodowaniem zmiennych kategorycznych, jak i z kodowaniem WOE. Wersje danych po procedurze PCA odłożone są w osobnych zbiorach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: liczba zmiennych przed PCA: 16, liczba zmiennych po PCA: 13\n",
      "Dataset 2: liczba zmiennych przed PCA: 21, liczba zmiennych po PCA: 16\n",
      "Dataset 3: liczba zmiennych przed PCA: 21, liczba zmiennych po PCA: 17\n",
      "Dataset 4: liczba zmiennych przed PCA: 21, liczba zmiennych po PCA: 17\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "\n",
    "expected_explained_variance_ratio = 0.95\n",
    "\n",
    "def pca_model(dataset):\n",
    "    pca = PCA(random_state=42)\n",
    "    pca.fit(dataset)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    counter = 0\n",
    "    explained_variance_cumulative = 0\n",
    "    for variance in explained_variance:\n",
    "        if explained_variance_cumulative <= expected_explained_variance_ratio:\n",
    "            explained_variance_cumulative += variance\n",
    "            counter += 1\n",
    "    pca = PCA(random_state=42, n_components = counter)\n",
    "    pca.fit(dataset)\n",
    "    return pca\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['PCA'] = pca_model(datasets[item]['X_train_scaled'])\n",
    "    datasets[item]['WOE_PCA'] = pca_model(datasets[item]['X_train_woe_scaled'])\n",
    "    datasets[item]['X_train_pca'] = datasets[item]['PCA'].transform(datasets[item]['X_train_scaled'])\n",
    "    datasets[item]['X_test_pca'] = datasets[item]['PCA'].transform(datasets[item]['X_test_scaled'])\n",
    "    datasets[item]['X_train_woe_pca'] = datasets[item]['WOE_PCA'].transform(datasets[item]['X_train_woe_scaled'])\n",
    "    datasets[item]['X_test_woe_pca'] = datasets[item]['WOE_PCA'].transform(datasets[item]['X_test_woe_scaled'])\n",
    "    print(f\"Dataset {item}: liczba zmiennych przed PCA: {datasets[item]['X_train'].shape[1]}, liczba zmiennych po PCA: {datasets[item]['X_train_pca'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podumowując - mamy 4 zbiory danych:\n",
    "\n",
    "    - 1 - usunięte zmienne z brakami,\n",
    "    - 2 - braki uzupełnione medianą,\n",
    "    - 3 - braki uzupełnione przez losowanie z rozkładu normalnego,\n",
    "    - 4 - braki uzupełnione przez kopiowanie danych z innych rekordów datasetu,\n",
    "\n",
    "    każdy zbiór w 6 wersjach:\n",
    "    \n",
    "    - bez skalowania, bez WOE,\n",
    "    - bez skalowania, z WOE,\n",
    "    - ze skalowaniem, bez WOE, bez PCA,\n",
    "    - ze skalowaniem, z WOE, bez PCA,\n",
    "    - ze skalowaniem, bez WOE, z PCA,\n",
    "    - ze skalowaniem, z WOE, z PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Modelowanie\n",
    "\n",
    "W modelowaniu przyjąłem następujacy schemat działania - dla danego klasyfikatora:\n",
    "    \n",
    "- zdefiniowanie zakresu parametrów klasyfikatora oraz zakresu wariantów danych do modelowania,\n",
    "- optymalizacja parametrów klasyfikatora z zastosowaniem Grid Search + Cross Validation,\n",
    "- wybór najlepszych zestawów parametrów,\n",
    "- powtórzenie modelowania na wybranych zestawach parametrów na pełnym zbiorze treningowym,\n",
    "- ocena wyników modeli - zestawienie miar dla zbioru testowego i treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja zwracająca miary jakości modelu dla zadanego modelu oraz zbioru zmiennych modelowych i obserwacji\n",
    "def get_measures(model, X, y):\n",
    "    y_predict = model.predict(X)\n",
    "    confusion = metrics.confusion_matrix(y, y_predict)\n",
    "    out = {}\n",
    "    out['accuracy'] = metrics.accuracy_score(y_predict, y)\n",
    "    out['precision'] = metrics.precision_score(y_predict, y)\n",
    "    out['recall'] = metrics.recall_score(y_predict, y)\n",
    "    out['f1'] = metrics.f1_score(y_predict, y)\n",
    "    out['auc'] = metrics.roc_auc_score(y, y_predict)\n",
    "    out['TP'] = confusion[1][1]\n",
    "    out['FP'] = confusion[1][0]\n",
    "    out['TN'] = confusion[0][0]\n",
    "    out['FN'] = confusion[0][1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja realizująca procedurę Grid Search + Cross Validation\n",
    "def model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1):\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=cv, scoring=scoring, n_jobs=n_jobs, verbose = verbose, refit=True)\n",
    "    grid_search.fit(X, y)\n",
    "    with open(prefix + '_grid_search.pickle', 'wb') as file:\n",
    "        pickle.dump(grid_search, file)\n",
    "    params_list =  ['param_'+x for x in param_grid]\n",
    "    params_list += ['params', 'mean_train_score', 'mean_test_score'] \n",
    "    result = pd.DataFrame(grid_search.cv_results_)[params_list]\n",
    "    result.to_csv(prefix + \".csv\", index=False, sep=';')\n",
    "    return grid_search, result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresja logistyczna\n",
    "\n",
    "Pierwszym stosowanym klasyfikatorem jest regresja logistczna.\n",
    "W procedurze Grid Search sprawdzane będą dwa parametry:\n",
    " \n",
    "- parametr C - parametr sterujący regularyzacją,\n",
    "- parametr 'class_weight' - parametr sterujący przeważaniem zmiennej objaśnianej.\n",
    "\n",
    "Konieczność zastosowania przeważania obserwacji pozytywnych dla zmiennej objaśnianej wynika z bardzo małego udzuału tych obserwacji w analizowanym zbiorze danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - pierwszy przebieg\n",
    "\n",
    "# klasyfikator, zestaw przeszukiwanych parametrów klasyfikatora\n",
    "\n",
    "classifier = LogisticRegression(C = 1.0, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "C = [10**c for c in range(1, 12, 2)]\n",
    "weights = [1, 50, 100, 200]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'C': C, 'class_weight': class_weight}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warianty danych badane per każdy dataset\n",
    "data_variants = ['', '_pca', '_scaled', '_woe', '_woe_scaled', '_woe_pca']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przebieg Grid Search:\n",
    "\n",
    "- per każdy dataset (4 wersje),\n",
    "- per każdy sposób przygotowania danych (6 wersji),\n",
    "- z optymalizacją miar: AUC / F-score.\n",
    "\n",
    "Z uwagi na czasochłonnośc przeliczenia wynik każdego przebiegu podlega serializacji i zapisowi na dysk, \n",
    "dodatkowo zbierane są średnie miary uzyskane w procedurze Cross Validation dla danego zestawu parametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_C', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'logistic_regression_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('logistic_regression_step_1.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej zestawienie parametrów dla 10 najlepszych uzyskanych wartości miar AUC i F1.\n",
    "Jakość uzyskiwanych modeli nie jest wysoka.\n",
    "\n",
    "Analiza przedstawionych danych pozwala stwierdzić, że:\n",
    "\n",
    "- różnice miar pomiędzy zbiorem testowym i treningowym są niewielkie (w granicach 1pp) --> model nie jest przeuczony,\n",
    "- jakość modelu nie zależy praktycznie od parametru C,\n",
    "- dla maksymalizacji AUC preferowane są zbiory z wagami WOE, dla maksymalizacji F-score - bez wag WOE,\n",
    "- dla obu miar preferowane są zbiory skalowane,\n",
    "- jakość modelu mocno zależy od parametru 'class_weight' - optymalna wartośc parametru to ok. 50 lub 'balanced'\n",
    "- w top 10 nie pojawiają się: \n",
    "    dataset 1 (= usunięte zmienne z brakami), \n",
    "    warianty danych z PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>scoring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>100000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>10000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>10</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>100000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>10000000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score       param_C  \\\n",
       "626  _woe_scaled        2         0.825199          0.829608  100000000000   \n",
       "611  _woe_scaled        2         0.825199          0.829608        100000   \n",
       "606  _woe_scaled        2         0.825199          0.829608          1000   \n",
       "621  _woe_scaled        2         0.825199          0.829608    1000000000   \n",
       "616  _woe_scaled        2         0.825199          0.829608      10000000   \n",
       "601  _woe_scaled        2         0.825199          0.829608            10   \n",
       "614  _woe_scaled        2         0.825147          0.829591        100000   \n",
       "619  _woe_scaled        2         0.825147          0.829591      10000000   \n",
       "624  _woe_scaled        2         0.825147          0.829591    1000000000   \n",
       "629  _woe_scaled        2         0.825147          0.829591  100000000000   \n",
       "\n",
       "    param_class_weight  scoring  \n",
       "626      {0: 1, 1: 50}  roc_auc  \n",
       "611      {0: 1, 1: 50}  roc_auc  \n",
       "606      {0: 1, 1: 50}  roc_auc  \n",
       "621      {0: 1, 1: 50}  roc_auc  \n",
       "616      {0: 1, 1: 50}  roc_auc  \n",
       "601      {0: 1, 1: 50}  roc_auc  \n",
       "614           balanced  roc_auc  \n",
       "619           balanced  roc_auc  \n",
       "624           balanced  roc_auc  \n",
       "629           balanced  roc_auc  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"logistic_regression_step_1.csv\", sep=';')\n",
    "results = results.drop(['params'], axis=1)\n",
    "\n",
    "results_best_auc = results[results['scoring']=='roc_auc']\n",
    "results_best_auc = results_best_auc.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results_best_auc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>scoring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087873</td>\n",
       "      <td>10</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>100000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>10000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087434</td>\n",
       "      <td>0.086708</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087433</td>\n",
       "      <td>0.086786</td>\n",
       "      <td>10000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.087602</td>\n",
       "      <td>10</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.087602</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score       param_C  \\\n",
       "516      _scaled        2         0.087597          0.087875          1000   \n",
       "531      _scaled        2         0.087597          0.087875    1000000000   \n",
       "511      _scaled        2         0.087597          0.087873            10   \n",
       "521      _scaled        2         0.087597          0.087875        100000   \n",
       "526      _scaled        2         0.087597          0.087875      10000000   \n",
       "536      _scaled        2         0.087597          0.087875  100000000000   \n",
       "756          NaN        3         0.087434          0.086708          1000   \n",
       "766          NaN        3         0.087433          0.086786      10000000   \n",
       "871      _scaled        3         0.087076          0.087602            10   \n",
       "876      _scaled        3         0.087076          0.087602          1000   \n",
       "\n",
       "    param_class_weight scoring  \n",
       "516      {0: 1, 1: 50}      f1  \n",
       "531      {0: 1, 1: 50}      f1  \n",
       "511      {0: 1, 1: 50}      f1  \n",
       "521      {0: 1, 1: 50}      f1  \n",
       "526      {0: 1, 1: 50}      f1  \n",
       "536      {0: 1, 1: 50}      f1  \n",
       "756      {0: 1, 1: 50}      f1  \n",
       "766      {0: 1, 1: 50}      f1  \n",
       "871      {0: 1, 1: 50}      f1  \n",
       "876      {0: 1, 1: 50}      f1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_best_f1 = results[results['scoring']=='f1']\n",
    "results_best_f1 = results_best_f1.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results_best_f1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'param_C', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "classifier = LogisticRegression(C = 10**5, class_weight = 'balanced', random_state = 42, verbose = 0)\n",
    "weights = [10, 30, 40, 50, 60, 70, 80, 90]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'class_weight': class_weight}\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        prefix = 'logistic_regression_2_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=4, n_jobs=11, verbose = 0)\n",
    "        datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('logistic_regression_step_2.csv', index=False, sep=';')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825215</td>\n",
       "      <td>0.829572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825174</td>\n",
       "      <td>0.829610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825143</td>\n",
       "      <td>0.829463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 30}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825125</td>\n",
       "      <td>0.829584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 70}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825055</td>\n",
       "      <td>0.829724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825052</td>\n",
       "      <td>0.829723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825049</td>\n",
       "      <td>0.829720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 70}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825038</td>\n",
       "      <td>0.829544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 80}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_variant  dataset  mean_test_score  mean_train_score  param_C  \\\n",
       "11  _woe_scaled        2         0.825215          0.829572      NaN   \n",
       "12  _woe_scaled        2         0.825199          0.829608      NaN   \n",
       "13  _woe_scaled        2         0.825174          0.829610      NaN   \n",
       "17  _woe_scaled        2         0.825147          0.829591      NaN   \n",
       "10  _woe_scaled        2         0.825143          0.829463      NaN   \n",
       "14  _woe_scaled        2         0.825125          0.829584      NaN   \n",
       "49  _woe_scaled        4         0.825055          0.829724      NaN   \n",
       "53  _woe_scaled        4         0.825052          0.829723      NaN   \n",
       "50  _woe_scaled        4         0.825049          0.829720      NaN   \n",
       "15  _woe_scaled        2         0.825038          0.829544      NaN   \n",
       "\n",
       "   param_class_weight  \n",
       "11      {0: 1, 1: 40}  \n",
       "12      {0: 1, 1: 50}  \n",
       "13      {0: 1, 1: 60}  \n",
       "17           balanced  \n",
       "10      {0: 1, 1: 30}  \n",
       "14      {0: 1, 1: 70}  \n",
       "49      {0: 1, 1: 60}  \n",
       "53           balanced  \n",
       "50      {0: 1, 1: 70}  \n",
       "15      {0: 1, 1: 80}  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"logistic_regression_step_2.csv\", sep=';')\n",
    "results = results.drop(['params'], axis=1)\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki drugiego przebiegu Grid Search wskazują na:\n",
    "\n",
    "- sposób przygotowania danych '_woe_scaled' jako zwracający najlepsze wyniki,\n",
    "- przewagę drugiego datasetu (uzupełnienie braków medianą),\n",
    "- zakres optywamllnych wartości dla przeważenia klas w zmiennej obserwowanej: 40-80,\n",
    "- brak przeuczenia - średni wynik na zbiorach uczących i testowych jest prawie taki sam.\n",
    "\n",
    "Kolejnym krokiem będzie wygenerowanie zestawu modeli z wykorzystaniem całego zbioru uczącego dla:\n",
    "- wszystkich datasetów,\n",
    "- wszystkich sposobów przygotowania danych,\n",
    "- dla różnych wartości parametrów 'class_weight' w zakresie wskazanym przez Grid Search.\n",
    "\n",
    "Dla każdego modelu zwrócony zostanie zestaw miar zarówno dla zbioru uczącego jak i testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'f1_test', \n",
    "                   'accuracy_train', 'accuracy_test', 'precision_train', 'precision_test', 'recall_train', \n",
    "                   'recall_test', 'TN_train', 'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', \n",
    "                   'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "                             \n",
    "weights = range(40, 80, 5)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "class_weights.append('balanced')\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for class_weight in class_weights:\n",
    "            X_train = datasets[dataset]['X_train' + data_variant]\n",
    "            y_train = datasets[dataset]['y_train']\n",
    "            X_test = datasets[dataset]['X_test' + data_variant]\n",
    "            y_test = datasets[dataset]['y_test']               \n",
    "            model = LogisticRegression(C = 10**5, class_weight = class_weight, random_state = 42, n_jobs = -1)\n",
    "            model.fit(X_train, y_train)\n",
    "            train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "            test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "            train = train.rename(columns = train_columns_rename)\n",
    "            test  = test.rename(columns = test_columns_rename)\n",
    "            result = pd.concat([train, test], axis = 1)\n",
    "            result['dataset'] = dataset\n",
    "            result['data_variant'] = data_variant\n",
    "            result['class_weight'] = class_weight[1]\n",
    "            result = result[dataset_columns]\n",
    "            model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('logistic_regression_modelling.csv', index=False, sep=';')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_variant</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>auc_train</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.760186</td>\n",
       "      <td>0.749607</td>\n",
       "      <td>0.073845</td>\n",
       "      <td>0.697142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.758953</td>\n",
       "      <td>0.746252</td>\n",
       "      <td>0.073582</td>\n",
       "      <td>0.696744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.758017</td>\n",
       "      <td>0.747078</td>\n",
       "      <td>0.073310</td>\n",
       "      <td>0.695917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.757929</td>\n",
       "      <td>0.747461</td>\n",
       "      <td>0.072992</td>\n",
       "      <td>0.693710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.757703</td>\n",
       "      <td>0.743965</td>\n",
       "      <td>0.075068</td>\n",
       "      <td>0.708511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>65</td>\n",
       "      <td>0.757601</td>\n",
       "      <td>0.744640</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.723558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>a</td>\n",
       "      <td>0.757573</td>\n",
       "      <td>0.744309</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>0.716387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.757005</td>\n",
       "      <td>0.745318</td>\n",
       "      <td>0.075045</td>\n",
       "      <td>0.709170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>_woe_pca</td>\n",
       "      <td>75</td>\n",
       "      <td>0.756693</td>\n",
       "      <td>0.751021</td>\n",
       "      <td>0.072054</td>\n",
       "      <td>0.688225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.756575</td>\n",
       "      <td>0.738725</td>\n",
       "      <td>0.074995</td>\n",
       "      <td>0.709339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset data_variant class_weight  auc_train  auc_test  f1_train  \\\n",
       "133        4  _woe_scaled           75   0.760186  0.749607  0.073845   \n",
       "97         3  _woe_scaled           75   0.758953  0.746252  0.073582   \n",
       "61         2  _woe_scaled           75   0.758017  0.747078  0.073310   \n",
       "25         1  _woe_scaled           75   0.757929  0.747461  0.072992   \n",
       "60         2  _woe_scaled           70   0.757703  0.743965  0.075068   \n",
       "131        4  _woe_scaled           65   0.757601  0.744640  0.077419   \n",
       "134        4  _woe_scaled            a   0.757573  0.744309  0.076255   \n",
       "132        4  _woe_scaled           70   0.757005  0.745318  0.075045   \n",
       "34         1     _woe_pca           75   0.756693  0.751021  0.072054   \n",
       "96         3  _woe_scaled           70   0.756575  0.738725  0.074995   \n",
       "\n",
       "     accuracy_train  \n",
       "133        0.697142  \n",
       "97         0.696744  \n",
       "61         0.695917  \n",
       "25         0.693710  \n",
       "60         0.708511  \n",
       "131        0.723558  \n",
       "134        0.716387  \n",
       "132        0.709170  \n",
       "34         0.688225  \n",
       "96         0.709339  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"logistic_regression_modelling.csv\", sep=';')\n",
    "results = results.sort_values(by=['auc_train'], ascending=False)\n",
    "results = results[['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej przedstawiono 10 najlepszych modeli (sortowanie wg AUC na zbiorze treningowym malejąco).\n",
    "Jakość modeli nie jest zadowalająca - niski f-score, dość niska wartość AUC --> siła predykcyjna modelu jest niska."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drzewa decyzyjne\n",
    "\n",
    "Kolejna próba wykorzystuje jako klasyfikator drzewa decyzyjne.\n",
    "Dla klasyfikatorów drzewiastych liniowe przeskalowanie danych jest transparentne i nie wpływa na wynik, dlatego nie badam wszystkich wariantów przygotowania danych jak w przypadku regresji logistycznej.\n",
    "\n",
    "Procedura optymalizacji modelu jest analogiczna jak w przypadku regresji logistycznej:\n",
    "- Grid Search + Cross Validation z przeszukaniem szerokiego zakresu parametrów klasyfikatora,\n",
    "- modelowanie dla najlepiej rokujących zestawów parametrów.\n",
    "\n",
    "Do optymalizacji uwzględniam:\n",
    "- maksymalną długość drzewa,\n",
    "- liczbę zmiennych branych pod uwagę w wyborze najlepszego podziału (parametr ten wprowadza element losowości),\n",
    "- wagi dla klas zmiennej objaśnianej (z uwagi na nierównomierną liczność klas).\n",
    "\n",
    "Uwzględniając doświadczenia z modelowania metodą regresji logistycznej pomijam pierwszy dataset w dalszych analizach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': [3, 5, 7, 9, 11, 15, 20],\n",
       " 'max_features': [None, 0.5, 0.75],\n",
       " 'class_weight': [{0: 1, 1: 30},\n",
       "  {0: 1, 1: 40},\n",
       "  {0: 1, 1: 50},\n",
       "  {0: 1, 1: 60},\n",
       "  {0: 1, 1: 70},\n",
       "  {0: 1, 1: 80},\n",
       "  {0: 1, 1: 90},\n",
       "  'balanced',\n",
       "  None]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=None, max_features=None, random_state=42, class_weight=None)\n",
    "\n",
    "max_depth = [3, 5, 7, 9, 11, 15, 20]\n",
    "max_features = [None, 0.5, 0.75]\n",
    "weights = [30, 40, 50, 60, 70, 80, 90]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "class_weight.append(None)\n",
    "param_grid = {'max_depth': max_depth, 'max_features': max_features, 'class_weight':class_weight}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_pca', '_scaled', '_woe_scaled', '_woe_pca']:\n",
    "        prefix = 'decision_tree_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=4, n_jobs=-1, verbose = 0)\n",
    "        datasets[dataset]['grid_search_decision_tree_' + data_variant + '_' + scoring] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('decision_tree_grid_search.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.822702</td>\n",
       "      <td>0.845665</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.822354</td>\n",
       "      <td>0.856692</td>\n",
       "      <td>{0: 1, 1: 90}</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 90}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.820863</td>\n",
       "      <td>0.845625</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.819775</td>\n",
       "      <td>0.840669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': None, 'max_depth': 5, 'max_fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.819454</td>\n",
       "      <td>0.847338</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 50}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.819181</td>\n",
       "      <td>0.858793</td>\n",
       "      <td>{0: 1, 1: 90}</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 90}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818528</td>\n",
       "      <td>0.846761</td>\n",
       "      <td>{0: 1, 1: 70}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 70}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.818021</td>\n",
       "      <td>0.847523</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 50}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.816765</td>\n",
       "      <td>0.862087</td>\n",
       "      <td>{0: 1, 1: 80}</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 80}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.816258</td>\n",
       "      <td>0.844222</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 5, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "1769      _scaled        4         0.822702          0.845665   \n",
       "1077      _scaled        3         0.822354          0.856692   \n",
       "1958  _woe_scaled        4         0.820863          0.845625   \n",
       "362       _scaled        2         0.819775          0.840669   \n",
       "1748      _scaled        4         0.819454          0.847338   \n",
       "321       _scaled        2         0.819181          0.858793   \n",
       "278       _scaled        2         0.818528          0.846761   \n",
       "1937  _woe_scaled        4         0.818021          0.847523   \n",
       "1056      _scaled        3         0.816765          0.862087   \n",
       "1853      _scaled        4         0.816258          0.844222   \n",
       "\n",
       "     param_class_weight  param_max_depth  param_max_features  \\\n",
       "1769      {0: 1, 1: 60}                5                0.75   \n",
       "1077      {0: 1, 1: 90}                7                 NaN   \n",
       "1958      {0: 1, 1: 60}                5                0.75   \n",
       "362                 NaN                5                0.75   \n",
       "1748      {0: 1, 1: 50}                5                0.75   \n",
       "321       {0: 1, 1: 90}                7                 NaN   \n",
       "278       {0: 1, 1: 70}                5                0.75   \n",
       "1937      {0: 1, 1: 50}                5                0.75   \n",
       "1056      {0: 1, 1: 80}                7                 NaN   \n",
       "1853           balanced                5                0.75   \n",
       "\n",
       "                                                 params  \n",
       "1769  {'class_weight': {0: 1, 1: 60}, 'max_depth': 5...  \n",
       "1077  {'class_weight': {0: 1, 1: 90}, 'max_depth': 7...  \n",
       "1958  {'class_weight': {0: 1, 1: 60}, 'max_depth': 5...  \n",
       "362   {'class_weight': None, 'max_depth': 5, 'max_fe...  \n",
       "1748  {'class_weight': {0: 1, 1: 50}, 'max_depth': 5...  \n",
       "321   {'class_weight': {0: 1, 1: 90}, 'max_depth': 7...  \n",
       "278   {'class_weight': {0: 1, 1: 70}, 'max_depth': 5...  \n",
       "1937  {'class_weight': {0: 1, 1: 50}, 'max_depth': 5...  \n",
       "1056  {'class_weight': {0: 1, 1: 80}, 'max_depth': 7...  \n",
       "1853  {'class_weight': 'balanced', 'max_depth': 5, '...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"decision_tree_grid_search.csv\", sep=';')\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizując uzyskane wyniki można stwierdzić, że:\n",
    "\n",
    "- wśród najwyżej ocenianych nie występują zbiory przygotowane z wykorzystaniem PCA,\n",
    "- różnica pomiędzy średnim wynikiem dla zbiorów testowych i treningowych nie przekracza 3pp --> modele nie są przeuczone,\n",
    "- optymalna wartośc parametru 'class_weight' znajduje się w granicach 60-90\n",
    "- optymalna wartość parametru 'max_depth' znajduje się w granicach 5-7\n",
    "- optymalna wartość parametru 'max_features' znajduje się w granicach 75-100%\n",
    "\n",
    "Kolejnym krokiem jest modelowanie w zakresach parametrów wskazanych przez Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drzewa decyzyjne - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'f1_test', \n",
    "                   'accuracy_train', 'accuracy_test', 'precision_train', 'precision_test', 'recall_train', \n",
    "                   'recall_test', 'TN_train', 'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', \n",
    "                   'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "                             \n",
    "weights = range(50, 90, 10)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "max_depths = [5, 6, 7]\n",
    "max_features_set = [0.75, None]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for max_depth in max_depths:\n",
    "            for max_features in max_features_set:\n",
    "                for class_weight in class_weights:\n",
    "                    X_train = datasets[dataset]['X_train' + data_variant]\n",
    "                    y_train = datasets[dataset]['y_train']\n",
    "                    X_test = datasets[dataset]['X_test' + data_variant]\n",
    "                    y_test = datasets[dataset]['y_test']               \n",
    "                    model = DecisionTreeClassifier(max_depth=max_depth, max_features=max_features, random_state=42, class_weight=class_weight)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "                    test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "                    train = train.rename(columns = train_columns_rename)\n",
    "                    test  = test.rename(columns = test_columns_rename)\n",
    "                    result = pd.concat([train, test], axis = 1)\n",
    "                    result['dataset'] = dataset\n",
    "                    result['data_variant'] = data_variant\n",
    "                    result['class_weight'] = class_weight[1]\n",
    "                    result = result[dataset_columns]\n",
    "                    model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('decision_tree_modelling.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"decision_tree_modelling.csv\", sep=';')\n",
    "results = results[results['auc_train'] - results['auc_test'] < 0.05]\n",
    "results = results.sort_values(by=['auc_train'], ascending=False)\n",
    "results = results[['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla modeli z najwyższą wartością AUC występuje znaczna różnica pomiędzy AUC dla zbioru testowego i treningowego, co wskazuje na przeuczenie modelu. Dlatego ograniczyłem analizowane wyniki wyłącznie do modeli, dla których różnica w ocenach zbioru testowego i treningowego nie przekracza 5pp AUC.\n",
    "\n",
    "Najlepszy model wykazuje 81% AUC przy bardzo słabej wartości f-score. Siła predykcyjna tego modelu jest słaba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Następnym klasyfikatorem użytym w modelowaniu są lasy losowe.\n",
    "Jest to naturalne rozszerzenie klasyfikacji za pomocą drzew.\n",
    "\n",
    "W optymalizacji korzystam z wypracowanego schematu postępowania, optymalizacji podlegają te same parametry, co w przypadku drzew decyzyjnych, dodatkowo optymalizuję parametr liczby estymatorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [100, 300, 400, 500, 700],\n",
       " 'max_depth': [5, 6, 7, 8, 9],\n",
       " 'max_features': [None, 0.75, 0.9],\n",
       " 'class_weight': [{0: 1, 1: 40},\n",
       "  {0: 1, 1: 60},\n",
       "  {0: 1, 1: 80},\n",
       "  'balanced',\n",
       "  None]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 100, max_depth=None, max_features=None, random_state=42, class_weight=None)\n",
    "\n",
    "n_estimators = [100, 300, 400, 500, 700]\n",
    "max_depth = [5, 6, 7, 8, 9]\n",
    "max_features = [None, 0.75, 0.90]\n",
    "weights = [40, 60, 80]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "class_weight.append(None)\n",
    "param_grid = {'n_estimators':n_estimators, 'max_depth': max_depth, 'max_features': max_features, 'class_weight':class_weight}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_2__scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 85.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_2__woe_scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 31.8min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 85.9min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_3__scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 44.6min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 79.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 120.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_3__woe_scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 44.6min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 79.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 120.9min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_4__scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 35.0min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 62.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 94.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_4__woe_scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 35.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 63.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 95.4min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        prefix = 'random_forest_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=3, n_jobs=-1, verbose = 1)\n",
    "        datasets[dataset]['grid_search_random_forest_' + data_variant] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('random_forest_grid_search.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.847175</td>\n",
       "      <td>0.912623</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.847089</td>\n",
       "      <td>0.912480</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.847078</td>\n",
       "      <td>0.912370</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846965</td>\n",
       "      <td>0.912489</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846962</td>\n",
       "      <td>0.912753</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846908</td>\n",
       "      <td>0.912417</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846857</td>\n",
       "      <td>0.912874</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846743</td>\n",
       "      <td>0.912551</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846630</td>\n",
       "      <td>0.911803</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.892889</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846530</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846458</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846453</td>\n",
       "      <td>0.893218</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846405</td>\n",
       "      <td>0.910021</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.90</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.904646</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.892897</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846384</td>\n",
       "      <td>0.904581</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846363</td>\n",
       "      <td>0.892891</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846353</td>\n",
       "      <td>0.892844</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846349</td>\n",
       "      <td>0.893216</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "411  _woe_scaled        2         0.847175          0.912623   \n",
       "37       _scaled        2         0.847089          0.912480   \n",
       "36       _scaled        2         0.847078          0.912370   \n",
       "38       _scaled        2         0.846965          0.912489   \n",
       "412  _woe_scaled        2         0.846962          0.912753   \n",
       "39       _scaled        2         0.846908          0.912417   \n",
       "413  _woe_scaled        2         0.846857          0.912874   \n",
       "414  _woe_scaled        2         0.846743          0.912551   \n",
       "35       _scaled        2         0.846630          0.911803   \n",
       "396  _woe_scaled        2         0.846581          0.892889   \n",
       "112      _scaled        2         0.846530          0.904700   \n",
       "410  _woe_scaled        2         0.846458          0.912178   \n",
       "397  _woe_scaled        2         0.846453          0.893218   \n",
       "44       _scaled        2         0.846405          0.910021   \n",
       "113      _scaled        2         0.846391          0.904646   \n",
       "24       _scaled        2         0.846389          0.892897   \n",
       "111      _scaled        2         0.846384          0.904581   \n",
       "23       _scaled        2         0.846363          0.892891   \n",
       "22       _scaled        2         0.846353          0.892844   \n",
       "398  _woe_scaled        2         0.846349          0.893216   \n",
       "\n",
       "    param_class_weight  param_max_depth  param_max_features  \\\n",
       "411      {0: 1, 1: 40}                7                0.75   \n",
       "37       {0: 1, 1: 40}                7                0.75   \n",
       "36       {0: 1, 1: 40}                7                0.75   \n",
       "38       {0: 1, 1: 40}                7                0.75   \n",
       "412      {0: 1, 1: 40}                7                0.75   \n",
       "39       {0: 1, 1: 40}                7                0.75   \n",
       "413      {0: 1, 1: 40}                7                0.75   \n",
       "414      {0: 1, 1: 40}                7                0.75   \n",
       "35       {0: 1, 1: 40}                7                0.75   \n",
       "396      {0: 1, 1: 40}                6                0.75   \n",
       "112      {0: 1, 1: 60}                7                0.75   \n",
       "410      {0: 1, 1: 40}                7                0.75   \n",
       "397      {0: 1, 1: 40}                6                0.75   \n",
       "44       {0: 1, 1: 40}                7                0.90   \n",
       "113      {0: 1, 1: 60}                7                0.75   \n",
       "24       {0: 1, 1: 40}                6                0.75   \n",
       "111      {0: 1, 1: 60}                7                0.75   \n",
       "23       {0: 1, 1: 40}                6                0.75   \n",
       "22       {0: 1, 1: 40}                6                0.75   \n",
       "398      {0: 1, 1: 40}                6                0.75   \n",
       "\n",
       "     param_n_estimators                                             params  \n",
       "411                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "37                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "36                  300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "38                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "412                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "39                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "413                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "414                 700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "35                  100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "396                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "112                 400  {'class_weight': {0: 1, 1: 60}, 'max_depth': 7...  \n",
       "410                 100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "397                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "44                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "113                 500  {'class_weight': {0: 1, 1: 60}, 'max_depth': 7...  \n",
       "24                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "111                 300  {'class_weight': {0: 1, 1: 60}, 'max_depth': 7...  \n",
       "23                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "22                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "398                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"random_forest_grid_search.csv\", sep=';')\n",
    "# results = results[results['mean_train_score'] - results['mean_test_score'] < 0.05]\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.892889</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846453</td>\n",
       "      <td>0.893218</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.892897</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846363</td>\n",
       "      <td>0.892891</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846353</td>\n",
       "      <td>0.892844</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846349</td>\n",
       "      <td>0.893216</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846344</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846337</td>\n",
       "      <td>0.893024</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846320</td>\n",
       "      <td>0.892534</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846244</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846142</td>\n",
       "      <td>0.891102</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846077</td>\n",
       "      <td>0.891628</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845967</td>\n",
       "      <td>0.890870</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845921</td>\n",
       "      <td>0.891595</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845839</td>\n",
       "      <td>0.891436</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845784</td>\n",
       "      <td>0.891572</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845746</td>\n",
       "      <td>0.891569</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845649</td>\n",
       "      <td>0.891602</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845612</td>\n",
       "      <td>0.891043</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845607</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "396  _woe_scaled        2         0.846581          0.892889   \n",
       "397  _woe_scaled        2         0.846453          0.893218   \n",
       "24       _scaled        2         0.846389          0.892897   \n",
       "23       _scaled        2         0.846363          0.892891   \n",
       "22       _scaled        2         0.846353          0.892844   \n",
       "398  _woe_scaled        2         0.846349          0.893216   \n",
       "20       _scaled        2         0.846344          0.892500   \n",
       "399  _woe_scaled        2         0.846337          0.893024   \n",
       "21       _scaled        2         0.846320          0.892534   \n",
       "395  _woe_scaled        2         0.846244          0.892707   \n",
       "26       _scaled        2         0.846142          0.891102   \n",
       "27       _scaled        2         0.846077          0.891628   \n",
       "25       _scaled        2         0.845967          0.890870   \n",
       "28       _scaled        2         0.845921          0.891595   \n",
       "29       _scaled        2         0.845839          0.891436   \n",
       "403  _woe_scaled        2         0.845784          0.891572   \n",
       "404  _woe_scaled        2         0.845746          0.891569   \n",
       "402  _woe_scaled        2         0.845649          0.891602   \n",
       "401  _woe_scaled        2         0.845612          0.891043   \n",
       "97       _scaled        2         0.845607          0.888336   \n",
       "\n",
       "    param_class_weight  param_max_depth  param_max_features  \\\n",
       "396      {0: 1, 1: 40}                6                0.75   \n",
       "397      {0: 1, 1: 40}                6                0.75   \n",
       "24       {0: 1, 1: 40}                6                0.75   \n",
       "23       {0: 1, 1: 40}                6                0.75   \n",
       "22       {0: 1, 1: 40}                6                0.75   \n",
       "398      {0: 1, 1: 40}                6                0.75   \n",
       "20       {0: 1, 1: 40}                6                0.75   \n",
       "399      {0: 1, 1: 40}                6                0.75   \n",
       "21       {0: 1, 1: 40}                6                0.75   \n",
       "395      {0: 1, 1: 40}                6                0.75   \n",
       "26       {0: 1, 1: 40}                6                0.90   \n",
       "27       {0: 1, 1: 40}                6                0.90   \n",
       "25       {0: 1, 1: 40}                6                0.90   \n",
       "28       {0: 1, 1: 40}                6                0.90   \n",
       "29       {0: 1, 1: 40}                6                0.90   \n",
       "403      {0: 1, 1: 40}                6                0.90   \n",
       "404      {0: 1, 1: 40}                6                0.90   \n",
       "402      {0: 1, 1: 40}                6                0.90   \n",
       "401      {0: 1, 1: 40}                6                0.90   \n",
       "97       {0: 1, 1: 60}                6                0.75   \n",
       "\n",
       "     param_n_estimators                                             params  \n",
       "396                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "397                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "24                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "23                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "22                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "398                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "20                  100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "399                 700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "21                  300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "395                 100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "26                  300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "27                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "25                  100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "28                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "29                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "403                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "404                 700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "402                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "401                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "97                  400  {'class_weight': {0: 1, 1: 60}, 'max_depth': 6...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"random_forest_grid_search.csv\", sep=';')\n",
    "results = results[results['mean_train_score'] - results['mean_test_score'] < 0.05]\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki miar jakości modeli uzyskanych w Grid Search w zależności od parametrów klasyfikatora posortowałem malejąco wg średniej wartości AUC dla zbioru testowego. Analizując wyniki zwrócone dla najlepszych 20 szacowań (powyżej) można stwierdzić, że: \n",
    "- róznica na AUC pomiędzy zbiorem testowym i treningowym przekracza 5pp, co może wskazywać na przeuczenie modeli,\n",
    "- występuje wyłącznie dataset 2,\n",
    "- jako waga dla klasy objaśnianej występuje prawie wyłącznie wartość 40, \n",
    "- parametr max_depth występuje w granicach 6-7,\n",
    "- parametr max_features występuje wyłącznie w wartości 0.75\n",
    "- dominuje 'class_weight' na poziomie 40\n",
    "- przeważa ;max_weight' na poziomie 75%,\n",
    "- jakość modelu jest nieczuła na liczbę klasyfikatorów w badanym zakresie\n",
    "\n",
    "Po wprowadzeniu ograniczenia w zbiorze wyników - dopuszczalna różnica AUC_train - AUC_test < 5% (wykluczenie modeli przeuczonych; wyniki poniżej):\n",
    "- wcześniejsze obserwacje pozostają w mocy.\n",
    "\n",
    "Dziwi brak czułości metody na parametr liczby estymatorów - prawdopodobnie przeszukiwany zestaw zawierał zbyt wysokie wartości. Podobnie wskazana wartość 'class_weight' to najniższa wartość w badanym zestawie. Niestety ze wzgledu na długi czas obliczeń nie byłem w stanie przetworzyć ponownie Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'n_estimators', 'max_depth', 'max_features', 'class_weight', \n",
    "                   'auc_train', 'auc_test', 'f1_train', 'f1_test', 'accuracy_train', 'accuracy_test', \n",
    "                   'precision_train', 'precision_test', 'recall_train', 'recall_test', 'TN_train', \n",
    "                   'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "\n",
    "n_estimators_set = [10, 50, 100, 300] \n",
    "weights = range(20, 60, 5)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "max_depths = [5, 6, 7, 8]\n",
    "max_features_set = [0.65, 0.75, 0.85]\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        for n_estimators in n_estimators_set:\n",
    "            for max_depth in max_depths:\n",
    "                for max_features in max_features_set:\n",
    "                    for class_weight in class_weights:\n",
    "                        X_train = datasets[dataset]['X_train' + data_variant]\n",
    "                        y_train = datasets[dataset]['y_train']\n",
    "                        X_test = datasets[dataset]['X_test' + data_variant]\n",
    "                        y_test = datasets[dataset]['y_test']               \n",
    "                        model = RandomForestClassifier(n_estimators = n_estimators, max_depth=max_depth, max_features=max_features, random_state=42, class_weight=class_weight, n_jobs=-1)\n",
    "                        model.fit(X_train, y_train)\n",
    "                        train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "                        test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "                        train = train.rename(columns = train_columns_rename)\n",
    "                        test  = test.rename(columns = test_columns_rename)\n",
    "                        result = pd.concat([train, test], axis = 1)\n",
    "                        result['dataset'] = dataset\n",
    "                        result['data_variant'] = data_variant\n",
    "                        result['n_estimators'] = n_estimators\n",
    "                        result['max_depth'] = max_depth\n",
    "                        result['max_features'] = max_features\n",
    "                        result['class_weight'] = class_weight[1]\n",
    "                        result = result[dataset_columns]\n",
    "                        model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('random_forest_modelling.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - druga przymiarka\n",
    "\n",
    "# klasyfikator + parametry do grid_searcha\n",
    "classifier = LogisticRegression(C = 10**5, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "weights = [30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'class_weight': class_weight}\n",
    "\n",
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'logistic_regression_2_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - trzecia przymiarka\n",
    "\n",
    "# klasyfikator + parametry do grid_searcha\n",
    "classifier = LogisticRegression(C = 10**5, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "weights = range(20, 100, 5)\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'class_weight': class_weight}\n",
    "\n",
    "data_variants = ['', '_scaled', '_woe_scaled']\n",
    "\n",
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'logistic_regression_3_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - czwarta przymiarka\n",
    "\n",
    "# klasyfikator + parametry do grid_searcha\n",
    "classifier = LogisticRegression(C = 10**5, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "weights = range(0, 20, 5)\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'class_weight': class_weight}\n",
    "\n",
    "data_variants = ['', '_scaled', '_woe_scaled']\n",
    "\n",
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'logistic_regression_4_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - piąta przymiarka\n",
    "\n",
    "# klasyfikator + parametry do grid_searcha\n",
    "classifier = LogisticRegression(C = 10**5, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "weights = range(10, 30, 1)\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'class_weight': class_weight}\n",
    "\n",
    "data_variants = ['', '_scaled', '_woe_scaled']\n",
    "\n",
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'logistic_regression_5_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = LogisticRegression(C = 10**5, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "# m.fit(datasets[1]['X_train'], datasets[1]['y_train'])\n",
    "# d = get_measures(m, datasets[1]['X_train'], datasets[1]['y_train'])\n",
    "d1 = pd.DataFrame.from_dict(d, orient = 'index').transpose()\n",
    "d2 = pd.DataFrame.from_dict(d, orient = 'index').transpose()\n",
    "d1 = d1.rename(columns = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', 'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', 'FN':'FN_train'})\n",
    "d2 = d2.rename(columns = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', 'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', 'FN':'FN_test'})\n",
    "df = pd.concat([d1, d2], axis = 1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'f1_test', \n",
    "                   'accuracy_train', 'accuracy_test', 'precision_train', 'precision_test', 'recall_train', 'recall_test',\n",
    "                   'TN_train', 'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', 'f1':'f1_train',\n",
    "                        'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', 'FN':'FN_train'}\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', 'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', 'FN':'FN_test'}\n",
    "                             \n",
    "weights = range(1, 80, 4)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "class_weights.append('balanced')\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for class_weight in class_weights:\n",
    "            X_train = datasets[dataset]['X_train' + data_variant]\n",
    "            y_train = datasets[dataset]['y_train']\n",
    "            X_test = datasets[dataset]['X_test' + data_variant]\n",
    "            y_test = datasets[dataset]['y_test']               \n",
    "            model = LogisticRegression(C = 10**5, class_weight = class_weight, random_state = 42, verbose = 1)\n",
    "            model.fit(X_train, y_train)\n",
    "            train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "            test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "            train = train.rename(columns = train_columns_rename)\n",
    "            test  = test.rename(columns = test_columns_rename)\n",
    "            result = pd.concat([train, test], axis = 1)\n",
    "            result['dataset'] = dataset\n",
    "            result['data_variant'] = data_variant\n",
    "            result['class_weight'] = class_weight[1]\n",
    "            result = result[dataset_columns]\n",
    "            model_results = pd.concat([model_results, result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'f1_test', \n",
    " 'accuracy_train', 'accuracy_test', 'precision_train', 'precision_test', 'recall_train', 'recall_test',\n",
    " 'TN_train', 'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'f1_test', \n",
    "                   'accuracy_train', 'accuracy_test', 'precision_train', 'precision_test', 'recall_train', 'recall_test',\n",
    "                   'TN_train', 'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', 'f1':'f1_train',\n",
    "                        'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', 'FN':'FN_train'}\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', 'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', 'FN':'FN_test'}\n",
    "                             \n",
    "weights = range(1, 80, 4)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "class_weights.append('balanced')\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for class_weight in class_weights:\n",
    "            X_train = datasets[dataset]['X_train' + data_variant]\n",
    "            y_train = datasets[dataset]['y_train']\n",
    "            X_test = datasets[dataset]['X_test' + data_variant]\n",
    "            y_test = datasets[dataset]['y_test']               \n",
    "            model = LogisticRegression(C = 10**5, class_weight = class_weight, random_state = 42, verbose = 1)\n",
    "            model.fit(X_train, y_train)\n",
    "            train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "            test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "            train = train.rename(columns = train_columns_rename)\n",
    "            test  = test.rename(columns = test_columns_rename)\n",
    "            result = pd.concat([train, test], axis = 1)\n",
    "            result['dataset'] = dataset\n",
    "            result['data_variant'] = data_variant\n",
    "            result['class_weight'] = class_weight[1]\n",
    "            result = result[dataset_columns]\n",
    "            model_results = pd.concat([model_results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.to_csv('logistic_regression_modelling.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost - pierwsza przymiarka\n",
    "\n",
    "# klasyfikator + parametry do grid_searcha\n",
    "classifier = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, verbosity=1, scale_pos_weight=1)\n",
    "\n",
    "max_depth = [3, 5, 7, 9, 11]\n",
    "learning_rate = [0.001, 0.01, 0.1, 1.]\n",
    "n_estimators = [50, 200, 500, 700]\n",
    "scale_pos_weight = [1, 10, 30, 50]\n",
    "\n",
    "param_grid = {'max_depth': max_depth, 'learning_rate':learning_rate, 'n_estimators':n_estimators, 'scale_pos_weight':scale_pos_weight}\n",
    "\n",
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_max_depth', 'param_learning_rate', \n",
    "                                  'param_n_estimators', 'param_scale_pos_weight',  'params', 'mean_train_score', \n",
    "                                  'mean_test_score'])\n",
    "\n",
    "data_variants = ['_scaled', '_woe_scaled']\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'xgb_1_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=11, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost - druga przymiarka\n",
    "\n",
    "# klasyfikator + parametry do grid_searcha\n",
    "classifier = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, verbosity=1, scale_pos_weight=1)\n",
    "\n",
    "max_depth = [3, 5, 7]\n",
    "learning_rate = [0.01, 0.1]\n",
    "n_estimators = [200, 400, 600]\n",
    "scale_pos_weight = [20, 40, 60]\n",
    "\n",
    "param_grid = {'max_depth': max_depth, 'learning_rate':learning_rate, 'n_estimators':n_estimators, 'scale_pos_weight':scale_pos_weight}\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_max_depth', 'param_learning_rate', \n",
    "                                  'param_n_estimators', 'param_scale_pos_weight',  'params', 'mean_train_score', \n",
    "                                  'mean_test_score'])\n",
    "\n",
    "\n",
    "data_variants = ['_scaled', '_woe_scaled']\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'xgb_2_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=3, n_jobs=11, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('gb_2.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# XGBoost - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'param_max_depth', 'param_n_estimators', 'param_scale_pos_weight',\n",
    "                   'auc_train', \n",
    "                   'auc_test', 'f1_train', 'f1_test', 'accuracy_train', 'accuracy_test', 'precision_train', \n",
    "                   'precision_test', 'recall_train', 'recall_test', 'TN_train', 'TN_test', 'FN_train', \n",
    "                   'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', 'f1':'f1_train',\n",
    "                        'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', 'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', 'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', 'FN':'FN_test'}\n",
    "                             \n",
    "max_depths = [3, 4, 5, 6]\n",
    "n_estimators_set = [400, 500, 600]\n",
    "scale_pos_weights = [30, 50, 70]\n",
    "\n",
    "data_variants = ['_pca', '_scaled', '_woe_scaled', '_woe_pca']\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for max_depth in max_depths:\n",
    "            for n_estimators in n_estimators_set:\n",
    "                for scale_pos_weight in scale_pos_weights:\n",
    "                    X_train = datasets[dataset]['X_train' + data_variant]\n",
    "                    y_train = datasets[dataset]['y_train']\n",
    "                    X_test = datasets[dataset]['X_test' + data_variant]\n",
    "                    y_test = datasets[dataset]['y_test']               \n",
    "                    model = xgboost.XGBClassifier(n_jobs=7, max_depth=max_depth, learning_rate=0.01, n_estimators=n_estimators, verbosity=1, scale_pos_weight=50)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "                    test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "                    train = train.rename(columns = train_columns_rename)\n",
    "                    test  = test.rename(columns = test_columns_rename)\n",
    "                    result = pd.concat([train, test], axis = 1)\n",
    "                    result['dataset'] = dataset\n",
    "                    result['data_variant'] = data_variant\n",
    "                    result['param_max_depth'] = max_depth\n",
    "                    result['param_n_estimators'] = n_estimators\n",
    "                    result['param_scale_pos_weight'] = scale_pos_weight\n",
    "                    result = result[dataset_columns]\n",
    "                    model_results = pd.concat([model_results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.to_csv('xgb_2_modelling.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [10**c for c in range(3, 9, 2)]\n",
    "weights = [1, 20, 50, 100, 200]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "\n",
    "max_depth = [4, 5, 7,  9]\n",
    "n_estimators = [10, 100, 200, 400]\n",
    "max_features = ['sqrt', None]\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1.]\n",
    "\n",
    "classifiers = {\n",
    "    'log_reg' : \n",
    "    {\n",
    "        'param_grid':{'C': C, 'class_weight': class_weight}, \n",
    "        'classifier':LogisticRegression(C = 1.0, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "    },\n",
    "    'tree' : \n",
    "    {\n",
    "        'param_grid':{'max_depth':max_depth, 'class_weight':class_weight}, \n",
    "        'classifier':DecisionTreeClassifier(random_state=42, splitter='best', max_depth=10, class_weight='balanced')\n",
    "    },\n",
    "    'random_forest' : \n",
    "    {\n",
    "        'param_grid':{'max_depth':max_depth, 'class_weight':class_weight, 'n_estimators':n_estimators},\n",
    "        'classifier':RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, class_weight='balanced', verbose=1)\n",
    "    },\n",
    "    'gradient_boosting' :\n",
    "    {\n",
    "        'param_grid':{'learning_rate':learning_rate, 'n_estimators':n_estimators, 'max_features':max_features},\n",
    "        'classifier':GradientBoostingClassifier(learning_rate = 0.01, n_estimators = 10, random_state=42, max_features=None, verbose=1)\n",
    "    },\n",
    "    'xgboost' :\n",
    "    {\n",
    "        'param_grid':{'learning_rate':learning_rate, 'n_estimators':n_estimators, 'max_features':max_features},\n",
    "        'classifier':xgboost.XGBClassifier(learning_rate = 0.01, n_estimators = 10, random_state=42, max_features=None, verbose=1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants = {\n",
    "    '1':{'X_train':set_1[0], 'y_train':set_1[2], 'X_test':set_1[1], 'y_test':set_1[3]},\n",
    "    '2':{'X_train':set_2[0], 'y_train':set_2[2], 'X_test':set_2[1], 'y_test':set_2[3]},\n",
    "    '3':{'X_train':set_3[0], 'y_train':set_3[2], 'X_test':set_3[1], 'y_test':set_3[3]},\n",
    "    '4':{'X_train':set_4[0], 'y_train':set_4[2], 'X_test':set_4[1], 'y_test':set_4[3]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_variants:\n",
    "    for model in classifiers:\n",
    "        model_grid_search(\n",
    "            data + '_' + model, \n",
    "            classifiers[model]['classifier'], \n",
    "            classifiers[model]['param_grid'], \n",
    "            data_variants[data]['X_train'], \n",
    "            data_variants[data]['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers['log_reg']['classifier'] = LogisticRegression(C = 1000.0, class_weight = {0: 1, 1: 200}, random_state = 42, verbose = 0)\n",
    "classifiers['tree']['classifier'] = DecisionTreeClassifier(random_state=42, splitter='best', max_depth=6, class_weight={0: 1, 1: 1})\n",
    "classifiers['random_forest']['classifier'] = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42, class_weight={0: 1, 1: 50})\n",
    "classifiers['gradient_boosting']['classifier'] = GradientBoostingClassifier(learning_rate = 0.05, n_estimators = 150, random_state=42, max_features=None, verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    classifiers[item]['classifier'].fit(X_train_scaled, y_train)\n",
    "    train_assessment = get_measures(classifiers[item]['classifier'], X_train_scaled, y_train)\n",
    "    test_assessment = get_measures(classifiers[item]['classifier'], X_test_scaled, y_test)\n",
    "    print(f\"{item} AUC_train = {'{:1.3f}'.format(train_assessment[-2])} AUC_test = {'{:1.3f}'.format(train_assessment[-2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers['log_reg']['classifier'] = LogisticRegression(C = 1000.0, class_weight = {0: 1, 1: 200}, random_state = 42, verbose = 0)\n",
    "classifiers['tree']['classifier'] = DecisionTreeClassifier(random_state=42, splitter='best', max_depth=6, class_weight={0: 1, 1: 1})\n",
    "classifiers['random_forest']['classifier'] = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42, class_weight={0: 1, 1: 50})\n",
    "classifiers['gradient_boosting']['classifier'] = GradientBoostingClassifier(learning_rate = 0.05, n_estimators = 150, random_state=42, max_features=None, verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    classifiers[item]['classifier'].fit(X_train_scaled_pca, y_train)\n",
    "    train_assessment = get_measures(classifiers[item]['classifier'], X_train_scaled_pca, y_train)\n",
    "    test_assessment = get_measures(classifiers[item]['classifier'], X_test_scaled_pca, y_test)\n",
    "    print(f\"{item} AUC_train = {'{:1.3f}'.format(train_assessment[-2])} AUC_test = {'{:1.3f}'.format(train_assessment[-2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [10**c for c in range(-3, 16, 1)]\n",
    "weights = [1, 2, 5, 10, 20, 30, 40, 50, 70, 100, 120, 150, 200, 250, 300, 400, 500]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "\n",
    "# 1 - Logistic Regression\n",
    "log_reg_param_grid = {'C': C, 'class_weight': class_weight}\n",
    "log_reg_classifier = LogisticRegression(C = 1.0, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "log_reg_grid_search = model_grid_search(log_reg_classifier, log_reg_param_grid, X_train, y_train)\n",
    "\n",
    "with open('log_reg_grid_search.pickle', 'wb') as file:\n",
    "    pickle.dump(log_reg_grid_search, file)\n",
    "    \n",
    "log_reg_result = pd.DataFrame(log_reg_grid_search.cv_results_)[['params', 'mean_train_score', 'mean_test_score']]\n",
    "log_reg_result.to_csv(\"log_reg_result.csv\", index=False, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(log_reg_grid_search.best_score_))\n",
    "HTML(pd.DataFrame(log_reg_grid_search.cv_results_)[['param_C', 'param_class_weight', 'mean_train_score', 'mean_test_score']].to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 - DecisionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_splitter = ['best']\n",
    "tree_max_depth = [3, 4, 5, 6, 7, 8]\n",
    "# tree_class_weight = [{0:1, 1:1}, {0:1, 1:50},{0:1, 1:100},{0:1, 1:150}, {0:1, 1:200}, {0:1, 1:250}, {0:1, 1:300}, 'balanced']\n",
    "tree_class_weight = class_weight\n",
    "\n",
    "tree_param_grid = {'splitter':tree_splitter, 'max_depth':tree_max_depth, 'class_weight':tree_class_weight}\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42, splitter='best', max_depth=10, class_weight='balanced')\n",
    "\n",
    "tree_grid_search = model_grid_search(tree_classifier, tree_param_grid, X_train, y_train)\n",
    "\n",
    "with open('tree_grid_search.pickle', 'wb') as file:\n",
    "    pickle.dump(tree_grid_search, file)\n",
    "\n",
    "tree_result = pd.DataFrame(tree_grid_search.cv_results_)[['param_class_weight', 'param_splitter', 'param_max_depth', 'mean_train_score', 'mean_test_score']]\n",
    "tree_result.to_csv(\"tree_result.csv\", index=False, sep = ';')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 - RandomForest\n",
    "\n",
    "rf_max_depth = [3, 5, 9, 15, 25]\n",
    "rf_class_weight = [{0:1, 1:1}, {0:1, 1:50}, {0:1, 1:100}, {0:1, 1:300}, 'balanced'] \n",
    "rf_n_estimators = [10, 50, 100, 500]\n",
    "rf_param_grid = {'max_depth':rf_max_depth, 'class_weight':rf_class_weight, 'n_estimators':rf_n_estimators}\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, class_weight='balanced')\n",
    "\n",
    "rf_grid_search = model_grid_search(rf_classifier, rf_param_grid, X_train, y_train)\n",
    "\n",
    "with open('rf_grid_search.pickle', 'wb') as file:\n",
    "    pickle.dump(rf_grid_search, file)\n",
    "\n",
    "rf_result = pd.DataFrame(rf_grid_search.cv_results_)[['param_class_weight', 'param_n_estimators', 'param_max_depth', 'mean_train_score', 'mean_test_score']]\n",
    "rf_result.to_csv(\"rf_result.csv\", index=False, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_learning_rate = [0.001, 0.01, 0.1, 1., 10.]\n",
    "gb_n_estimators = [20, 50, 100, 200, 300, 700]\n",
    "gb_max_features = ['sqrt', 'log2', None]\n",
    "gb_param_grid = {'learning_rate':gb_learning_rate, 'n_estimators':gb_n_estimators, 'max_features':gb_max_features}\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate = 0.01, n_estimators = 10, random_state=42, max_features=None, verbose=1)\n",
    "\n",
    "gb_grid_search = model_grid_search(gb_classifier, gb_param_grid, X_train, y_train)\n",
    "\n",
    "with open('gb_grid_search.pickle', 'wb') as file:\n",
    "    pickle.dump(gb_grid_search, file)                 \n",
    "\n",
    "gb_result = pd.DataFrame(gb_grid_search.cv_results_)[['param_learning_rate', 'param_n_estimators', 'param_max_features', 'mean_train_score', 'mean_test_score']]\n",
    "gb_result.to_csv(\"gb_result.csv\", index=False, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - XGBoost\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_learning_rate = [0.001, 0.01, 0.1, 1.]\n",
    "xgb_n_estimators = [20, 50, 100, 200, 300, 500, 700]\n",
    "xgb_max_features = ['sqrt', 'log2', None]\n",
    "xgb_param_grid = {'learning_rate':xgb_learning_rate, 'n_estimators':xgb_n_estimators, 'max_features':xgb_max_features}\n",
    "xgb_classifier = GradientBoostingClassifier(learning_rate = 0.01, n_estimators = 10, random_state=42, max_features=None, verbose=1) \n",
    "\n",
    "xgb_grid_search = model_grid_search(xgb_classifier, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "with open('xgb_grid_search.pickle', 'wb') as file:\n",
    "    pickle.dump(xgb_grid_search, file)                 \n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_grid_search.cv_results_)[['param_learning_rate', 'param_n_estimators', 'param_max_features', 'mean_train_score', 'mean_test_score']]\n",
    "xgb_result.to_csv(\"xgb_result.csv\", index=False, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(rf_grid_search.best_score_))\n",
    "HTML(pd.DataFrame(rf_grid_search.cv_results_)[['param_max_depth', 'param_class_weight', 'param_n_estimators', 'mean_train_score', 'mean_test_score']].to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "log_reg_C = [10**c for c in range(0, 10, 1)]\n",
    "for C in log_reg_C:\n",
    "    params = [C, weight]\n",
    "    model = LogisticRegression(C=C, class_weight=weight)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append([params, model, get_measures(model, X_train, y_train)[7:], get_measures(model, X_test, y_test)[7:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=42)\n",
    "pca.fit(X_train_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0\n",
    "explained_variance_cum = []\n",
    "for item in explained_variance:\n",
    "    ratio += item\n",
    "    explained_variance_cum.append(ratio)\n",
    "explained_variance_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 14, random_state=42)\n",
    "pca.fit(X_train_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0\n",
    "explained_variance_cum = []\n",
    "for item in explained_variance:\n",
    "    ratio += item\n",
    "    explained_variance_cum.append(ratio)\n",
    "explained_variance_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_pca = pca.transform(X_train_scaled)\n",
    "X_test_scaled_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Logistic Regression\n",
    "log_reg_C = [10**c for c in range(4, 11, 1)]\n",
    "log_reg_class_weight = [{0:1, 1:1}, {0:1, 1:50},{0:1, 1:100},{0:1, 1:150}, {0:1, 1:200}, {0:1, 1:250}, {0:1, 1:300}, 'balanced'] \n",
    "log_reg_param_grid = {'C': log_reg_C, 'class_weight': log_reg_class_weight}\n",
    "log_reg_grid_search = model_grid_search(LogisticRegression(), log_reg_param_grid, X_train_scaled_pca, y_train)\n",
    "\n",
    "with open('log_reg_grid_search_pca.pickle', 'wb') as file:\n",
    "    pickle.dump(log_reg_grid_search, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(log_reg_grid_search.best_score_))\n",
    "HTML(pd.DataFrame(log_reg_grid_search.cv_results_)[['param_C', 'param_class_weight', 'mean_train_score', 'mean_test_score']].to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "scores = []\n",
    "for item in models:\n",
    "    scores.append(item[2][1])\n",
    "scores = np.reshape(scores, (15,9))\n",
    "heatmap(scores, ylabel='C', yticklabels=log_reg_C, xlabel='weight', xticklabels=[item[1] for item in log_reg_class_weight], cmap=\"viridis\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(results.mean_test_score).reshape(6, 6)\n",
    "# plot the mean cross-validation scores\n",
    "heatmap(scores, xlabel='gamma', xticklabels=param_grid['gamma'], ylabel='C', yticklabels=param_grid['C'], cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n",
    "            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\"):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # plot the mean cross-validation scores\n",
    "    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n",
    "    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n",
    "                               img.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.mean(color[:3]) > 0.5:\n",
    "            c = 'k'\n",
    "        else:\n",
    "            c = 'w'\n",
    "        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in models:\n",
    "    print(f\"C = {item[0][0]}  weight = {item[0][1][1]}  auc_train = {item[2][1]}  auc_test = {item[3][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = []\n",
    "for C in [3**c for c in range(-15, 25, 1)]:\n",
    "    model = LogisticRegression(C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append([C, get_measures(model, X_train, y_train), get_measures(model, X_test, y_test)])\n",
    "\n",
    "C = [item[0] for item in scores]\n",
    "train = [item[1] for item in scores]\n",
    "test = [item[2] for item in scores]\n",
    "df_train = pd.DataFrame(train, columns = ['TN_train', 'FN_train', 'FP_train', 'TP_train', 'accuracy_train', 'precision_train', 'recall_train', 'f1_train', 'auc_train'])\n",
    "df_test = pd.DataFrame(test, columns = ['TN_test', 'FN_test', 'FP_test', 'TP_test', 'accuracy_test', 'precision_test', 'recall_test', 'f1_test', 'auc_test'])\n",
    "df = pd.concat([pd.DataFrame(C, columns = ['C']), df_train, df_test], axis=1)\n",
    "df.to_csv(\"Linear_regression_scores.csv\", sep = ';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost\n",
    "\n",
    "xgb_scores = []\n",
    "for max_depth in [3, 4, 5, 7, 9, 11]:\n",
    "    for learning_rate in [0.01, 0.1, 0.3, 0.5, 1, 3]:\n",
    "        for n_estimators in [200, 500, 700, 1000]:\n",
    "            xgb = xgboost.XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "            xgb.fit(X_train, y_train)\n",
    "            xgb_scores.append([ [max_depth, learning_rate, n_estimators], get_measures(model, X_train, y_train), get_measures(model, X_test, y_test)])\n",
    "\n",
    "hiperparams = [item[0] for item in xgb_scores]\n",
    "train = [item[1] for item in xgb_scores]\n",
    "test = [item[2] for item in xgb_scores]\n",
    "df_params = pd.DataFrame(hiperparams, columns=['max_depth', 'learning_rate', 'n_estimators'])\n",
    "df_train = pd.DataFrame(train, columns = ['TN_train', 'FN_train', 'FP_train', 'TP_train', 'accuracy_train', 'precision_train', 'recall_train', 'f1_train', 'auc_train'])\n",
    "df_test = pd.DataFrame(test, columns = ['TN_test', 'FN_test', 'FP_test', 'TP_test', 'accuracy_test', 'precision_test', 'recall_test', 'f1_test', 'auc_test'])\n",
    "df = pd.concat([df_params, df_train, df_test], axis=1)\n",
    "df.to_csv(\"xgboost_scores.csv\", sep = ';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model, X_train, y_train, X_test, y_test):\n",
    "    scores = {}\n",
    "    confusion_matrix_train = confusion_matrix(model.predict(X_train),y_train)\n",
    "    confusion_matrix_test = confusion_matrix(model.predict(X_test),y_test)\n",
    "    accuracy_score_train = accuracy_score(model.predict(X_train),y_train)\n",
    "    accuracy_score_test = accuracy_score(model.predict(X_test),y_test)\n",
    "    f1_score_train = f1_score(model.predict(X_train),y_train)\n",
    "    f1_score_test = f1_score(model.predict(X_test),y_test)\n",
    "\n",
    "    scores['train'] = {'TN':confusion_matrix_train[0][0], 'FP':confusion_matrix_train[0][1], \n",
    "                       'FN':confusion_matrix_train[1][0], 'TP':confusion_matrix_train[1][1],\n",
    "                        'accuracy':accuracy_score_train, 'F1_score':f1_score_train}\n",
    "\n",
    "    scores['test'] = {'TN':confusion_matrix_test[0][0], 'FP':confusion_matrix_test[0][1], \n",
    "                       'FN':confusion_matrix_test[1][0], 'TP':confusion_matrix_test[1][1],\n",
    "                        'accuracy':accuracy_score_test, 'F1_score':f1_score_test}\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = list(X_train[y_train==1].index)\n",
    "negative = list(X_train[y_train==0].index)\n",
    "\n",
    "positive_size = 20000\n",
    "negative_size = 180000\n",
    "\n",
    "positive_random = np.random.uniform(0, len(positive), positive_size)\n",
    "positive_random = [ positive[int(x)] for x in positive_random]\n",
    "positive_random = [X_train[X_train.index == x] for x in positive_random]\n",
    "positive_random = pd.concat(positive_random)\n",
    "positive_random['y'] = 1\n",
    "\n",
    "negative_random = np.random.uniform(0, len(negative), negative_size)\n",
    "negative_random = [ negative[int(x)] for x in negative_random]\n",
    "negative_random = [X_train[X_train.index == x] for x in negative_random]\n",
    "negative_random = pd.concat(negative_random)\n",
    "negative_random['y'] = 0\n",
    "\n",
    "X_train_bootstrap = pd.concat([positive_random, negative_random])\n",
    "X_train_bootstrap = X_train_bootstrap.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "y_train_bootstrap = X_train_bootstrap['y']\n",
    "X_train_bootstrap = X_train_bootstrap.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_scaler = StandardScaler()\n",
    "bootstrap_scaler.fit(X_train_bootstrap)\n",
    "X_train_bootstrap_scaled = bootstrap_scaler.transform(X_train_bootstrap)\n",
    "X_test_bootstrap_scaled = bootstrap_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sety danych:\n",
    "\n",
    "plain = X_train, y_train, X_test, y_test\n",
    "scaled = X_train_scaled, y_train, X_test_scaled, y_test\n",
    "bootstrap = X_train_bootstrap, y_train_bootstrap, X_test, y_test\n",
    "bootrstrap_scaled = X_train_bootstrap_scaled, y_train_bootstrap, X_test_bootstrap_scaled, y_test\n",
    "\n",
    "data_sets = {'plain':plain, 'scaled':scaled, 'boot':bootstrap, 'boot_scal':bootrstrap_scaled}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model_plain(model, X_train, y_train, X_test, y_test):\n",
    "    scores = {}\n",
    "    confusion_matrix_train = confusion_matrix(model.predict(X_train),y_train)\n",
    "    confusion_matrix_test = confusion_matrix(model.predict(X_test),y_test)\n",
    "    accuracy_score_train = accuracy_score(model.predict(X_train),y_train)\n",
    "    accuracy_score_test = accuracy_score(model.predict(X_test),y_test)\n",
    "    f1_score_train = f1_score(model.predict(X_train),y_train)\n",
    "    f1_score_test = f1_score(model.predict(X_test),y_test)\n",
    "\n",
    "    scores['train'] = {'TN_train':confusion_matrix_train[0][0], 'FP_train':confusion_matrix_train[0][1], \n",
    "                       'FN_train':confusion_matrix_train[1][0], 'TP_train':confusion_matrix_train[1][1],\n",
    "                        'accuracy_train':accuracy_score_train, 'F1_score_train':f1_score_train}\n",
    "\n",
    "    scores['test'] = {'TN_test':confusion_matrix_test[0][0], 'FP_test':confusion_matrix_test[0][1], \n",
    "                       'FN_test':confusion_matrix_test[1][0], 'TP_test':confusion_matrix_test[1][1],\n",
    "                        'accuracy_test':accuracy_score_test, 'F1_score_test':f1_score_test}\n",
    "    scores['train'].update(scores['test'])\n",
    "    return scores['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna\n",
    "\n",
    "models = []\n",
    "counter = 1\n",
    "for data_set in data_sets:\n",
    "    for C_param in range(-4, 14, 2):\n",
    "        model = {}\n",
    "        model['counter'] = counter\n",
    "        model['data_set'] = data_set\n",
    "        model['model'] = LogisticRegression(C=10**C_param)\n",
    "        ds = data_sets[data_set]\n",
    "        model['model'].fit(ds[0], ds[1])\n",
    "        model['C'] = C_param\n",
    "        assessment = assess_model(model['model'], ds[0], ds[1], ds[2], ds[3])\n",
    "        model.update(assessment)\n",
    "        models.append(model)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['train']['F1_score'] for x in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna na bootstrapie\n",
    "log_reg_bootstrap = LogisticRegression()\n",
    "log_reg_bootstrap.fit(X_train_bootstrap, y_train_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model(log_reg_bootstrap, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_scaled = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model(log_reg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model(log_reg_scaled, X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost\n",
    "\n",
    "xgb = xgboost.XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost\n",
    "\n",
    "xgb_bootstrap = xgboost.XGBClassifier()\n",
    "xgb_bootstrap.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "assess_model(xgb_bootstrap, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model(xgb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost\n",
    "\n",
    "# xgb_bootrstrap_scaled = xgboost.XGBClassifier()\n",
    "# xgb_bootrstrap_scaled.fit(bootrstrap_scaled[0], bootrstrap_scaled[1])\n",
    "assess_model(xgb_bootrstrap_scaled, bootrstrap_scaled[0], bootrstrap_scaled[1], bootrstrap_scaled[2], bootrstrap_scaled[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
