{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2 - modelowanie\n",
    "\n",
    "#### kolejne kroki:\n",
    "- pobranie danych do modelowania\n",
    "- przygotowanie zestawów danych train-test w kilku wariantach\n",
    "- modelowanie z zastosowaniem wybranych klasyfikatorów\n",
    "- omówienie i podsumowanie wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pakietów\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Przygotowanie danych do modelowania\n",
    "\n",
    "#### Import danych\n",
    "\n",
    "Dane do modelowania przygotowane zostały w 4 wersjach - wersje te różnią się sposobem uzupełnienia brakujących wartości w zakresie 5 zmiennych:\n",
    "\n",
    "- 'offer_amount', \n",
    "- 'offer_period', \n",
    "- 'interest_rate', \n",
    "- 'fee', \n",
    "- 'offer_monthly_obligation'.\n",
    "\n",
    "Są to następujące warianty:\n",
    "\n",
    "- 1 - zmienne usunięte ze zbioru danych,\n",
    "- 2 - braki danych uzupełnione medianą,\n",
    "- 3 - braki danych uzupełnione przez losowanie z rozkładu normalnego,\n",
    "- 4 - braki danych uzupełnione przez kopiowanie danych z innych rekordów datasetu.\n",
    "\n",
    "Przygotowanie modelu wykonywane będzie dla każdego z wariantów niezależnie.\n",
    "Sposób uzupełnienia wartości brakujacych zostanie oceniony po analizie jakości uzyskanych modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import danych\n",
    "col_names = ['ID', 'gender', 'city', 'income', 'birth_date', 'application_date', 'requested_amount', \n",
    "             'requested_period', 'financial_obligations', 'employer_name', 'account_bank', \n",
    "             'mobile_verification_flag', 'var_5', 'var_1', 'offer_amount', 'offer_period', 'interest_rate', \n",
    "             'fee', 'offer_monthly_obligation', 'filled_form_flag', 'device', 'var_2', 'source', 'var_4', \n",
    "             'disbursed_flag', 'latitude', 'longitude', 'age']\n",
    "\n",
    "datasets = {\n",
    "    1:{'name':'none'},\n",
    "    2:{'name':'median'},\n",
    "    3:{'name':'distribution'},\n",
    "    4:{'name':'observation'}\n",
    "}\n",
    "\n",
    "datasets[2]['data'] = pd.read_csv('dataset_inputation_median.csv', delimiter = ';', engine='python', header = None, names = col_names, \n",
    "                      index_col = 0, skiprows = 1 )\n",
    "datasets[3]['data'] = pd.read_csv('dataset_inputation_distribution.csv', delimiter = ';', engine='python', header = None, names = col_names, \n",
    "                      index_col = 0, skiprows = 1 )\n",
    "datasets[4]['data'] = pd.read_csv('dataset_inputation_random_observation.csv', delimiter = ';', engine='python', header = None, names = col_names, \n",
    "                      index_col = 0, skiprows = 1 )\n",
    "datasets[1]['data'] = datasets[2]['data'].copy()\n",
    "\n",
    "columns_to_drop = ['city', 'birth_date', 'application_date', 'employer_name', 'account_bank']\n",
    "columns_to_rename = {'ID':'id', \n",
    "                     'gender':'cat01', \n",
    "                     'income':'num01', \n",
    "                     'requested_amount':'num02', \n",
    "                     'requested_period':'num03',\n",
    "                     'financial_obligations':'num04',\n",
    "                     'mobile_verification_flag':'cat02',\n",
    "                     'var_5':'cat03',\n",
    "                     'var_1':'cat04',\n",
    "                     'offer_amount':'num05',\n",
    "                     'offer_period':'num06',\n",
    "                     'interest_rate':'num07',\n",
    "                     'fee':'num08',\n",
    "                     'offer_monthly_obligation':'num09',\n",
    "                     'filled_form_flag':'cat05',\n",
    "                     'device':'cat06',\n",
    "                     'var_2':'cat07',\n",
    "                     'source':'cat08',\n",
    "                     'var_4':'cat09',\n",
    "                     'disbursed_flag':'explained',\n",
    "                     'latitude':'num10',\n",
    "                     'longitude':'num11',\n",
    "                     'age':'num12'\n",
    "}\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['data'] = datasets[item]['data'].drop(columns_to_drop, axis=1).rename(columns=columns_to_rename)\n",
    "\n",
    "datasets[1]['data'] = datasets[1]['data'].drop(['num05','num06','num07','num08','num09'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podział train-test\n",
    "\n",
    "Dla każdego wariantu danych dzielę dataset na zbiór trenujacy i testowy wg proporcji 3:1.\n",
    "Klasa pozytywnych obserwacji zmiennej objaśnianej jest mało liczna, dlatego żeby zachować proporcjonalny podział\n",
    "stosuję opcję 'stratifiy'. \n",
    "\n",
    "Mała liczność obserwacji pozytywnych w zmiennej objaśnianej będzie wymagać zastosowania skalowania w procesie modelowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "licznosc_zbioru = 87020\n",
      "liczność obserwacji pozytywnych dla zmiennej objaśnianej = 1273.0\n",
      "udział obserwacji pozytywnych zmiennej objaśnianej w zbiorze = 0.01462882096069869\n",
      "\n",
      "Dataset 1: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n",
      "Dataset 2: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n",
      "Dataset 3: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n",
      "Dataset 4: train - 0.014632651497739983 'jedynek', test - 0.01461732934957481 'jedynek'\n"
     ]
    }
   ],
   "source": [
    "licznosc = datasets[1]['data'].shape[0]\n",
    "licznosc_y = sum(datasets[1]['data']['explained'])\n",
    "licznosc_y_procent = licznosc_y / licznosc\n",
    "print(f\"licznosc_zbioru = {licznosc}\")\n",
    "print(f\"liczność obserwacji pozytywnych dla zmiennej objaśnianej = {licznosc_y}\")\n",
    "print(f\"udział obserwacji pozytywnych zmiennej objaśnianej w zbiorze = {licznosc_y_procent}\\n\")\n",
    "      \n",
    "# Train - test split\n",
    "train_test_split_ratio = 0.25\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['X'] = datasets[item]['data'].drop(['explained'], axis=1)\n",
    "    datasets[item]['y'] = datasets[item]['data']['explained']\n",
    "    datasets[item]['X_train'], datasets[item]['X_test'], datasets[item]['y_train'], datasets[item]['y_test'] = train_test_split(datasets[item]['X'], datasets[item]['y'], test_size=train_test_split_ratio, random_state=42, stratify=datasets[item]['y'])\n",
    "    y_train_share = sum(datasets[item]['y_train'])/datasets[item]['X_train'].shape[0]\n",
    "    y_test_share  = sum(datasets[item]['y_test']) /datasets[item]['X_test'].shape[0]\n",
    "    print(f\"Dataset {item}: train - {y_train_share} 'jedynek', test - {y_test_share} 'jedynek'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podejście do zmiennych kategorycznych\n",
    "\n",
    "Na etapie przygotowania danych zmienne kategoryczne zostały zakodowane wg rosnącego udziału obserwacji pozytywnych w danej klasie. Bardziej adekwatne podejście (szczególnie w celu zastosowania regresji logistycznej) jest zastosowanie współczynników WOE (Weight Of Evidence), czyli skalowanie wg proporcji ln(liczba obserwacji pozytywnych / liczba obserwacji negatywnych). \n",
    "\n",
    "Dla każdego ze zbiorów danych wyznaczam wagi WOE do zmiany kodowania zmiennych kategorycznych. Do wyliczenia wag wykorzystuję wyłącznie obserwacje ze zbioru treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# WOE calculation for categorical\n",
    "categorical_variables = [name for name in datasets[1]['data'].columns if 'cat' in name]\n",
    "\n",
    "for item in datasets:\n",
    "    WOE_dataset = datasets[item]['X_train'][categorical_variables]\n",
    "    WOE_dataset['positive'] = datasets[item]['y_train']\n",
    "    WOE_dataset['negative'] = WOE_dataset.apply(lambda x: 1 - x['positive'], axis=1)\n",
    "    WOE_mapper = pd.DataFrame(columns=['feature_name', 'feature_code', 'WOE'])\n",
    "    for variable in categorical_variables:\n",
    "        variable_data = WOE_dataset[[variable, 'positive', 'negative']]\n",
    "        out = variable_data.groupby([variable]).sum()\n",
    "        out['WOE'] = out.apply(lambda x: log(x['positive'] / x['negative']) if x['positive'] > 0 and x['negative'] > 0 else -12, axis = 1)\n",
    "        out['feature_name'] = variable\n",
    "        out['feature_code'] = out.index\n",
    "        out = out[['feature_name', 'feature_code', 'WOE']].reset_index(drop=True)\n",
    "        WOE_mapper = pd.concat([WOE_mapper, out])\n",
    "    datasets[item]['WOE_mapper'] = WOE_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodowanie stosuję zarówno do zbioru treningowego, jak i testowego. Wariant danych z kodowaniem WOE oznaczam odrębnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with WOE\n",
    "\n",
    "def WOE_mapping(dataset, WOE_mapper):\n",
    "    output = dataset.copy()\n",
    "    for variable in categorical_variables:\n",
    "        mapper = WOE_mapper[WOE_mapper['feature_name'] == variable]\n",
    "        mapper_dict = {}\n",
    "        for category in mapper.index:\n",
    "            mapper_dict[mapper.loc[category]['feature_code']] = mapper.loc[category]['WOE']\n",
    "        output[variable] = output.apply(lambda x: mapper_dict[x[variable]], axis=1)\n",
    "    return output\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['X_train_woe'] = WOE_mapping(datasets[item]['X_train'], datasets[item]['WOE_mapper'])\n",
    "    datasets[item]['X_test_woe'] = WOE_mapping(datasets[item]['X_test'], datasets[item]['WOE_mapper'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standaryzacja\n",
    "\n",
    "Dane numeryczne w zbiorze danych są różnorodnie skalowane - np. wiek klienta to zmienna z przedziału ok. [0, 100], tymczasem kwota dochodu - już sięga milionów (dochód wyrażony jest w rupiach indyjskich). Zniwelowanie różnic w skali umożliwia zastosowanie standaryzacji zmiennych - stosuję do tego celu wbudowaną funkcjonalność StandardScaler(). \n",
    "\n",
    "Standaryzacji poddaję zarówno zbiory ze zwyczajnym kodowaniem zmiennych kategorycznych, jak i z kodowaniem WOE.\n",
    "Zbiory przeskalowane oznaczam odrębnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scaled\n",
    "for item in datasets:\n",
    "    datasets[item]['scaler'] = StandardScaler()\n",
    "    datasets[item]['WOE_scaler'] = StandardScaler()\n",
    "    datasets[item]['scaler'].fit(datasets[item]['X_train'])\n",
    "    datasets[item]['WOE_scaler'].fit(datasets[item]['X_train_woe'])\n",
    "    datasets[item]['X_train_scaled'] = datasets[item]['scaler'].transform(datasets[item]['X_train'])\n",
    "    datasets[item]['X_test_scaled'] = datasets[item]['scaler'].transform(datasets[item]['X_test'])\n",
    "    datasets[item]['X_train_woe_scaled'] = datasets[item]['WOE_scaler'].transform(datasets[item]['X_train_woe'])\n",
    "    datasets[item]['X_test_woe_scaled'] = datasets[item]['WOE_scaler'].transform(datasets[item]['X_test_woe'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redukcja wymiarowości\n",
    "\n",
    "Korelacja części zmiennych datasetu ze zmienną objaśnianą może być na tyle niska, że zmienna nie wniesie znaczącego udziału w jakość modelu. Redukcja wymiarowości umozliwia wyłuskanie najistotniejszych zmiennych do budowy modelu. Ograniczenie ilości danych usprawnia proces modelowania kosztem niewielkiej straty jakości modelu. Ma to szczególne znaczenie dla zbiorów ze sporą ilością zmiennych modelowych.\n",
    "\n",
    "W naszym przypadku redukcja wymiarowości nie jest konieczna z uwagi na niedużą liczbę zmiennych modleowych (poniżej 20). Mimo to wyznaczam zbiór danych z odcięciem części zmiennych - warunkiem jest obniżenie wyjaśnianej wariancji maksymalnie o 5%. Wykorzystuję Principal Component Analysis (jest dostępna wbudowana funkcjonalność w bibliotece sklearn).\n",
    "\n",
    "Procedurze PCA poddaje się zbiory po regularyzacji, wyliczenie stosuję zarówno do zbioru ze zwyczajnym kodowaniem zmiennych kategorycznych, jak i z kodowaniem WOE. Wersje danych po procedurze PCA odłożone są w osobnych zbiorach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: liczba zmiennych przed PCA: 16, liczba zmiennych po PCA: 13\n",
      "Dataset 2: liczba zmiennych przed PCA: 21, liczba zmiennych po PCA: 16\n",
      "Dataset 3: liczba zmiennych przed PCA: 21, liczba zmiennych po PCA: 17\n",
      "Dataset 4: liczba zmiennych przed PCA: 21, liczba zmiennych po PCA: 17\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "\n",
    "expected_explained_variance_ratio = 0.95\n",
    "\n",
    "def pca_model(dataset):\n",
    "    pca = PCA(random_state=42)\n",
    "    pca.fit(dataset)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    counter = 0\n",
    "    explained_variance_cumulative = 0\n",
    "    for variance in explained_variance:\n",
    "        if explained_variance_cumulative <= expected_explained_variance_ratio:\n",
    "            explained_variance_cumulative += variance\n",
    "            counter += 1\n",
    "    pca = PCA(random_state=42, n_components = counter)\n",
    "    pca.fit(dataset)\n",
    "    return pca\n",
    "\n",
    "for item in datasets:\n",
    "    datasets[item]['PCA'] = pca_model(datasets[item]['X_train_scaled'])\n",
    "    datasets[item]['WOE_PCA'] = pca_model(datasets[item]['X_train_woe_scaled'])\n",
    "    datasets[item]['X_train_pca'] = datasets[item]['PCA'].transform(datasets[item]['X_train_scaled'])\n",
    "    datasets[item]['X_test_pca'] = datasets[item]['PCA'].transform(datasets[item]['X_test_scaled'])\n",
    "    datasets[item]['X_train_woe_pca'] = datasets[item]['WOE_PCA'].transform(datasets[item]['X_train_woe_scaled'])\n",
    "    datasets[item]['X_test_woe_pca'] = datasets[item]['WOE_PCA'].transform(datasets[item]['X_test_woe_scaled'])\n",
    "    print(f\"Dataset {item}: liczba zmiennych przed PCA: {datasets[item]['X_train'].shape[1]}, liczba zmiennych po PCA: {datasets[item]['X_train_pca'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podumowując - mamy 4 zbiory danych:\n",
    "\n",
    "    - 1 - usunięte zmienne z brakami,\n",
    "    - 2 - braki uzupełnione medianą,\n",
    "    - 3 - braki uzupełnione przez losowanie z rozkładu normalnego,\n",
    "    - 4 - braki uzupełnione przez kopiowanie danych z innych rekordów datasetu,\n",
    "\n",
    "    każdy zbiór w 6 wersjach:\n",
    "    \n",
    "    - bez skalowania, bez WOE,\n",
    "    - bez skalowania, z WOE,\n",
    "    - ze skalowaniem, bez WOE, bez PCA,\n",
    "    - ze skalowaniem, z WOE, bez PCA,\n",
    "    - ze skalowaniem, bez WOE, z PCA,\n",
    "    - ze skalowaniem, z WOE, z PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Modelowanie\n",
    "\n",
    "W modelowaniu przyjąłem następujacy schemat działania - dla danego klasyfikatora:\n",
    "    \n",
    "- zdefiniowanie zakresu parametrów klasyfikatora oraz zakresu wariantów danych do modelowania,\n",
    "- optymalizacja parametrów klasyfikatora z zastosowaniem Grid Search + Cross Validation,\n",
    "- wybór najlepszych zestawów parametrów,\n",
    "- powtórzenie modelowania na wybranych zestawach parametrów na pełnym zbiorze treningowym,\n",
    "- ocena wyników modeli - zestawienie miar dla zbioru testowego i treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja zwracająca miary jakości modelu dla zadanego modelu oraz zbioru zmiennych modelowych i obserwacji\n",
    "def get_measures(model, X, y):\n",
    "    y_predict = model.predict(X)\n",
    "    confusion = metrics.confusion_matrix(y, y_predict)\n",
    "    out = {}\n",
    "    out['accuracy'] = metrics.accuracy_score(y_predict, y)\n",
    "    out['precision'] = metrics.precision_score(y_predict, y)\n",
    "    out['recall'] = metrics.recall_score(y_predict, y)\n",
    "    out['f1'] = metrics.f1_score(y_predict, y)\n",
    "    out['auc'] = metrics.roc_auc_score(y, y_predict)\n",
    "    out['TP'] = confusion[1][1]\n",
    "    out['FP'] = confusion[1][0]\n",
    "    out['TN'] = confusion[0][0]\n",
    "    out['FN'] = confusion[0][1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja realizująca procedurę Grid Search + Cross Validation\n",
    "def model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1):\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=cv, scoring=scoring, n_jobs=n_jobs, verbose = verbose, refit=True)\n",
    "    grid_search.fit(X, y)\n",
    "    with open(prefix + '_grid_search.pickle', 'wb') as file:\n",
    "        pickle.dump(grid_search, file)\n",
    "    params_list =  ['param_'+x for x in param_grid]\n",
    "    params_list += ['params', 'mean_train_score', 'mean_test_score'] \n",
    "    result = pd.DataFrame(grid_search.cv_results_)[params_list]\n",
    "    result.to_csv(prefix + \".csv\", index=False, sep=';')\n",
    "    return grid_search, result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Regresja logistyczna\n",
    "\n",
    "Pierwszym stosowanym klasyfikatorem jest regresja logistczna.\n",
    "W procedurze Grid Search sprawdzane będą dwa parametry:\n",
    " \n",
    "- parametr C - parametr sterujący regularyzacją,\n",
    "- parametr 'class_weight' - parametr sterujący przeważaniem zmiennej objaśnianej.\n",
    "\n",
    "Konieczność zastosowania przeważania obserwacji pozytywnych dla zmiennej objaśnianej wynika z bardzo małego udzuału tych obserwacji w analizowanym zbiorze danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - pierwszy przebieg\n",
    "\n",
    "# klasyfikator, zestaw przeszukiwanych parametrów klasyfikatora\n",
    "\n",
    "classifier = LogisticRegression(C = 1.0, class_weight = 'balanced', random_state = 42, verbose = 1)\n",
    "C = [10**c for c in range(1, 12, 2)]\n",
    "weights = [1, 50, 100, 200]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'C': C, 'class_weight': class_weight}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warianty danych badane per każdy dataset\n",
    "data_variants = ['', '_pca', '_scaled', '_woe', '_woe_scaled', '_woe_pca']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przebieg Grid Search:\n",
    "\n",
    "- per każdy dataset (4 wersje),\n",
    "- per każdy sposób przygotowania danych (6 wersji),\n",
    "- z optymalizacją miar: AUC / F-score.\n",
    "\n",
    "Z uwagi na czasochłonnośc przeliczenia wynik każdego przebiegu podlega serializacji i zapisowi na dysk, \n",
    "dodatkowo zbierane są średnie miary uzyskane w procedurze Cross Validation dla danego zestawu parametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'scoring', 'param_C', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for scoring in ['roc_auc', 'f1']:\n",
    "            prefix = 'logistic_regression_' + str(dataset) + '_' + data_variant + '_' + scoring\n",
    "            print(\"start: \" + prefix)\n",
    "            X = datasets[dataset]['X_train' + data_variant]\n",
    "            y = datasets[dataset]['y_train']\n",
    "            grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring, cv=4, n_jobs=7, verbose = 1)\n",
    "            datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "            result['dataset'], result['data_variant'], result['scoring'] = str(dataset), data_variant, scoring\n",
    "            results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('logistic_regression_step_1.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej zestawienie parametrów dla 10 najlepszych uzyskanych wartości miar AUC i F1.\n",
    "Jakość uzyskiwanych modeli nie jest wysoka.\n",
    "\n",
    "Analiza przedstawionych danych pozwala stwierdzić, że:\n",
    "\n",
    "- różnice miar pomiędzy zbiorem testowym i treningowym są niewielkie (w granicach 1pp) --> model nie jest przeuczony,\n",
    "- jakość modelu nie zależy praktycznie od parametru C,\n",
    "- dla maksymalizacji AUC preferowane są zbiory z wagami WOE, dla maksymalizacji F-score - bez wag WOE,\n",
    "- dla obu miar preferowane są zbiory skalowane,\n",
    "- jakość modelu mocno zależy od parametru 'class_weight' - optymalna wartośc parametru to ok. 50 lub 'balanced'\n",
    "- w top 10 nie pojawiają się: \n",
    "    dataset 1 (= usunięte zmienne z brakami), \n",
    "    warianty danych z PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>scoring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>100000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>10000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>10</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>100000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>10000000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>balanced</td>\n",
       "      <td>roc_auc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score       param_C  \\\n",
       "626  _woe_scaled        2         0.825199          0.829608  100000000000   \n",
       "611  _woe_scaled        2         0.825199          0.829608        100000   \n",
       "606  _woe_scaled        2         0.825199          0.829608          1000   \n",
       "621  _woe_scaled        2         0.825199          0.829608    1000000000   \n",
       "616  _woe_scaled        2         0.825199          0.829608      10000000   \n",
       "601  _woe_scaled        2         0.825199          0.829608            10   \n",
       "614  _woe_scaled        2         0.825147          0.829591        100000   \n",
       "619  _woe_scaled        2         0.825147          0.829591      10000000   \n",
       "624  _woe_scaled        2         0.825147          0.829591    1000000000   \n",
       "629  _woe_scaled        2         0.825147          0.829591  100000000000   \n",
       "\n",
       "    param_class_weight  scoring  \n",
       "626      {0: 1, 1: 50}  roc_auc  \n",
       "611      {0: 1, 1: 50}  roc_auc  \n",
       "606      {0: 1, 1: 50}  roc_auc  \n",
       "621      {0: 1, 1: 50}  roc_auc  \n",
       "616      {0: 1, 1: 50}  roc_auc  \n",
       "601      {0: 1, 1: 50}  roc_auc  \n",
       "614           balanced  roc_auc  \n",
       "619           balanced  roc_auc  \n",
       "624           balanced  roc_auc  \n",
       "629           balanced  roc_auc  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"logistic_regression_step_1.csv\", sep=';')\n",
    "results = results.drop(['params'], axis=1)\n",
    "\n",
    "results_best_auc = results[results['scoring']=='roc_auc']\n",
    "results_best_auc = results_best_auc.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results_best_auc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>scoring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087873</td>\n",
       "      <td>10</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>100000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>10000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087875</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087434</td>\n",
       "      <td>0.086708</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087433</td>\n",
       "      <td>0.086786</td>\n",
       "      <td>10000000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.087602</td>\n",
       "      <td>10</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.087602</td>\n",
       "      <td>1000</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score       param_C  \\\n",
       "516      _scaled        2         0.087597          0.087875          1000   \n",
       "531      _scaled        2         0.087597          0.087875    1000000000   \n",
       "511      _scaled        2         0.087597          0.087873            10   \n",
       "521      _scaled        2         0.087597          0.087875        100000   \n",
       "526      _scaled        2         0.087597          0.087875      10000000   \n",
       "536      _scaled        2         0.087597          0.087875  100000000000   \n",
       "756          NaN        3         0.087434          0.086708          1000   \n",
       "766          NaN        3         0.087433          0.086786      10000000   \n",
       "871      _scaled        3         0.087076          0.087602            10   \n",
       "876      _scaled        3         0.087076          0.087602          1000   \n",
       "\n",
       "    param_class_weight scoring  \n",
       "516      {0: 1, 1: 50}      f1  \n",
       "531      {0: 1, 1: 50}      f1  \n",
       "511      {0: 1, 1: 50}      f1  \n",
       "521      {0: 1, 1: 50}      f1  \n",
       "526      {0: 1, 1: 50}      f1  \n",
       "536      {0: 1, 1: 50}      f1  \n",
       "756      {0: 1, 1: 50}      f1  \n",
       "766      {0: 1, 1: 50}      f1  \n",
       "871      {0: 1, 1: 50}      f1  \n",
       "876      {0: 1, 1: 50}      f1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_best_f1 = results[results['scoring']=='f1']\n",
    "results_best_f1 = results_best_f1.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results_best_f1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'param_C', 'param_class_weight', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "classifier = LogisticRegression(C = 10**5, class_weight = 'balanced', random_state = 42, verbose = 0)\n",
    "weights = [10, 30, 40, 50, 60, 70, 80, 90]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "param_grid = {'class_weight': class_weight}\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        prefix = 'logistic_regression_2_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=4, n_jobs=11, verbose = 0)\n",
    "        datasets[dataset]['grid_search_logistic_regression_' + data_variant + '_' + scoring] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('logistic_regression_step_2.csv', index=False, sep=';')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_class_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825215</td>\n",
       "      <td>0.829572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825199</td>\n",
       "      <td>0.829608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825174</td>\n",
       "      <td>0.829610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.829591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825143</td>\n",
       "      <td>0.829463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 30}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825125</td>\n",
       "      <td>0.829584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 70}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825055</td>\n",
       "      <td>0.829724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825052</td>\n",
       "      <td>0.829723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825049</td>\n",
       "      <td>0.829720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 70}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825038</td>\n",
       "      <td>0.829544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{0: 1, 1: 80}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_variant  dataset  mean_test_score  mean_train_score  param_C  \\\n",
       "11  _woe_scaled        2         0.825215          0.829572      NaN   \n",
       "12  _woe_scaled        2         0.825199          0.829608      NaN   \n",
       "13  _woe_scaled        2         0.825174          0.829610      NaN   \n",
       "17  _woe_scaled        2         0.825147          0.829591      NaN   \n",
       "10  _woe_scaled        2         0.825143          0.829463      NaN   \n",
       "14  _woe_scaled        2         0.825125          0.829584      NaN   \n",
       "49  _woe_scaled        4         0.825055          0.829724      NaN   \n",
       "53  _woe_scaled        4         0.825052          0.829723      NaN   \n",
       "50  _woe_scaled        4         0.825049          0.829720      NaN   \n",
       "15  _woe_scaled        2         0.825038          0.829544      NaN   \n",
       "\n",
       "   param_class_weight  \n",
       "11      {0: 1, 1: 40}  \n",
       "12      {0: 1, 1: 50}  \n",
       "13      {0: 1, 1: 60}  \n",
       "17           balanced  \n",
       "10      {0: 1, 1: 30}  \n",
       "14      {0: 1, 1: 70}  \n",
       "49      {0: 1, 1: 60}  \n",
       "53           balanced  \n",
       "50      {0: 1, 1: 70}  \n",
       "15      {0: 1, 1: 80}  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"logistic_regression_step_2.csv\", sep=';')\n",
    "results = results.drop(['params'], axis=1)\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki drugiego przebiegu Grid Search wskazują na:\n",
    "\n",
    "- sposób przygotowania danych '_woe_scaled' jako zwracający najlepsze wyniki,\n",
    "- przewagę drugiego datasetu (uzupełnienie braków medianą),\n",
    "- zakres optywamllnych wartości dla przeważenia klas w zmiennej obserwowanej: 40-80,\n",
    "- brak przeuczenia - średni wynik na zbiorach uczących i testowych jest prawie taki sam.\n",
    "\n",
    "Kolejnym krokiem będzie wygenerowanie zestawu modeli z wykorzystaniem całego zbioru uczącego dla:\n",
    "- wszystkich datasetów,\n",
    "- wszystkich sposobów przygotowania danych,\n",
    "- dla różnych wartości parametrów 'class_weight' w zakresie wskazanym przez Grid Search.\n",
    "\n",
    "Dla każdego modelu zwrócony zostanie zestaw miar zarówno dla zbioru uczącego jak i testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja logistyczna - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'f1_test', \n",
    "                   'accuracy_train', 'accuracy_test', 'precision_train', 'precision_test', 'recall_train', \n",
    "                   'recall_test', 'TN_train', 'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', \n",
    "                   'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "                             \n",
    "weights = range(40, 80, 5)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "class_weights.append('balanced')\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for class_weight in class_weights:\n",
    "            X_train = datasets[dataset]['X_train' + data_variant]\n",
    "            y_train = datasets[dataset]['y_train']\n",
    "            X_test = datasets[dataset]['X_test' + data_variant]\n",
    "            y_test = datasets[dataset]['y_test']               \n",
    "            model = LogisticRegression(C = 10**5, class_weight = class_weight, random_state = 42, n_jobs = -1)\n",
    "            model.fit(X_train, y_train)\n",
    "            train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "            test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "            train = train.rename(columns = train_columns_rename)\n",
    "            test  = test.rename(columns = test_columns_rename)\n",
    "            result = pd.concat([train, test], axis = 1)\n",
    "            result['dataset'] = dataset\n",
    "            result['data_variant'] = data_variant\n",
    "            result['class_weight'] = class_weight[1]\n",
    "            result = result[dataset_columns]\n",
    "            model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('logistic_regression_modelling.csv', index=False, sep=';')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_variant</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>auc_train</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.760186</td>\n",
       "      <td>0.749607</td>\n",
       "      <td>0.073845</td>\n",
       "      <td>0.697142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.758953</td>\n",
       "      <td>0.746252</td>\n",
       "      <td>0.073582</td>\n",
       "      <td>0.696744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.758017</td>\n",
       "      <td>0.747078</td>\n",
       "      <td>0.073310</td>\n",
       "      <td>0.695917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>75</td>\n",
       "      <td>0.757929</td>\n",
       "      <td>0.747461</td>\n",
       "      <td>0.072992</td>\n",
       "      <td>0.693710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.757703</td>\n",
       "      <td>0.743965</td>\n",
       "      <td>0.075068</td>\n",
       "      <td>0.708511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>65</td>\n",
       "      <td>0.757601</td>\n",
       "      <td>0.744640</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.723558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>a</td>\n",
       "      <td>0.757573</td>\n",
       "      <td>0.744309</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>0.716387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.757005</td>\n",
       "      <td>0.745318</td>\n",
       "      <td>0.075045</td>\n",
       "      <td>0.709170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>_woe_pca</td>\n",
       "      <td>75</td>\n",
       "      <td>0.756693</td>\n",
       "      <td>0.751021</td>\n",
       "      <td>0.072054</td>\n",
       "      <td>0.688225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.756575</td>\n",
       "      <td>0.738725</td>\n",
       "      <td>0.074995</td>\n",
       "      <td>0.709339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset data_variant class_weight  auc_train  auc_test  f1_train  \\\n",
       "133        4  _woe_scaled           75   0.760186  0.749607  0.073845   \n",
       "97         3  _woe_scaled           75   0.758953  0.746252  0.073582   \n",
       "61         2  _woe_scaled           75   0.758017  0.747078  0.073310   \n",
       "25         1  _woe_scaled           75   0.757929  0.747461  0.072992   \n",
       "60         2  _woe_scaled           70   0.757703  0.743965  0.075068   \n",
       "131        4  _woe_scaled           65   0.757601  0.744640  0.077419   \n",
       "134        4  _woe_scaled            a   0.757573  0.744309  0.076255   \n",
       "132        4  _woe_scaled           70   0.757005  0.745318  0.075045   \n",
       "34         1     _woe_pca           75   0.756693  0.751021  0.072054   \n",
       "96         3  _woe_scaled           70   0.756575  0.738725  0.074995   \n",
       "\n",
       "     accuracy_train  \n",
       "133        0.697142  \n",
       "97         0.696744  \n",
       "61         0.695917  \n",
       "25         0.693710  \n",
       "60         0.708511  \n",
       "131        0.723558  \n",
       "134        0.716387  \n",
       "132        0.709170  \n",
       "34         0.688225  \n",
       "96         0.709339  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"logistic_regression_modelling.csv\", sep=';')\n",
    "results = results.sort_values(by=['auc_train'], ascending=False)\n",
    "results = results[['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej przedstawiono 10 najlepszych modeli (sortowanie wg AUC na zbiorze treningowym malejąco).\n",
    "Jakość modeli nie jest zadowalająca - niski f-score, dość niska wartość AUC --> siła predykcyjna modelu jest niska."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Drzewa decyzyjne\n",
    "\n",
    "Kolejna próba wykorzystuje jako klasyfikator drzewa decyzyjne.\n",
    "Dla klasyfikatorów drzewiastych liniowe przeskalowanie danych jest transparentne i nie wpływa na wynik, dlatego nie badam wszystkich wariantów przygotowania danych jak w przypadku regresji logistycznej.\n",
    "\n",
    "Procedura optymalizacji modelu jest analogiczna jak w przypadku regresji logistycznej:\n",
    "- Grid Search + Cross Validation z przeszukaniem szerokiego zakresu parametrów klasyfikatora,\n",
    "- modelowanie dla najlepiej rokujących zestawów parametrów.\n",
    "\n",
    "Do optymalizacji uwzględniam:\n",
    "- maksymalną długość drzewa,\n",
    "- liczbę zmiennych branych pod uwagę w wyborze najlepszego podziału (parametr ten wprowadza element losowości),\n",
    "- wagi dla klas zmiennej objaśnianej (z uwagi na nierównomierną liczność klas).\n",
    "\n",
    "Uwzględniając doświadczenia z modelowania metodą regresji logistycznej pomijam pierwszy dataset w dalszych analizach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': [3, 5, 7, 9, 11, 15, 20],\n",
       " 'max_features': [None, 0.5, 0.75],\n",
       " 'class_weight': [{0: 1, 1: 30},\n",
       "  {0: 1, 1: 40},\n",
       "  {0: 1, 1: 50},\n",
       "  {0: 1, 1: 60},\n",
       "  {0: 1, 1: 70},\n",
       "  {0: 1, 1: 80},\n",
       "  {0: 1, 1: 90},\n",
       "  'balanced',\n",
       "  None]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=None, max_features=None, random_state=42, class_weight=None)\n",
    "\n",
    "max_depth = [3, 5, 7, 9, 11, 15, 20]\n",
    "max_features = [None, 0.5, 0.75]\n",
    "weights = [30, 40, 50, 60, 70, 80, 90]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "class_weight.append(None)\n",
    "param_grid = {'max_depth': max_depth, 'max_features': max_features, 'class_weight':class_weight}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_pca', '_scaled', '_woe_scaled', '_woe_pca']:\n",
    "        prefix = 'decision_tree_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=4, n_jobs=-1, verbose = 0)\n",
    "        datasets[dataset]['grid_search_decision_tree_' + data_variant + '_' + scoring] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('decision_tree_grid_search.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.822702</td>\n",
       "      <td>0.845665</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.822354</td>\n",
       "      <td>0.856692</td>\n",
       "      <td>{0: 1, 1: 90}</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 90}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.820863</td>\n",
       "      <td>0.845625</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.819775</td>\n",
       "      <td>0.840669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': None, 'max_depth': 5, 'max_fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.819454</td>\n",
       "      <td>0.847338</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 50}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.819181</td>\n",
       "      <td>0.858793</td>\n",
       "      <td>{0: 1, 1: 90}</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 90}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818528</td>\n",
       "      <td>0.846761</td>\n",
       "      <td>{0: 1, 1: 70}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 70}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.818021</td>\n",
       "      <td>0.847523</td>\n",
       "      <td>{0: 1, 1: 50}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 50}, 'max_depth': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.816765</td>\n",
       "      <td>0.862087</td>\n",
       "      <td>{0: 1, 1: 80}</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 80}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.816258</td>\n",
       "      <td>0.844222</td>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 5, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "1769      _scaled        4         0.822702          0.845665   \n",
       "1077      _scaled        3         0.822354          0.856692   \n",
       "1958  _woe_scaled        4         0.820863          0.845625   \n",
       "362       _scaled        2         0.819775          0.840669   \n",
       "1748      _scaled        4         0.819454          0.847338   \n",
       "321       _scaled        2         0.819181          0.858793   \n",
       "278       _scaled        2         0.818528          0.846761   \n",
       "1937  _woe_scaled        4         0.818021          0.847523   \n",
       "1056      _scaled        3         0.816765          0.862087   \n",
       "1853      _scaled        4         0.816258          0.844222   \n",
       "\n",
       "     param_class_weight  param_max_depth  param_max_features  \\\n",
       "1769      {0: 1, 1: 60}                5                0.75   \n",
       "1077      {0: 1, 1: 90}                7                 NaN   \n",
       "1958      {0: 1, 1: 60}                5                0.75   \n",
       "362                 NaN                5                0.75   \n",
       "1748      {0: 1, 1: 50}                5                0.75   \n",
       "321       {0: 1, 1: 90}                7                 NaN   \n",
       "278       {0: 1, 1: 70}                5                0.75   \n",
       "1937      {0: 1, 1: 50}                5                0.75   \n",
       "1056      {0: 1, 1: 80}                7                 NaN   \n",
       "1853           balanced                5                0.75   \n",
       "\n",
       "                                                 params  \n",
       "1769  {'class_weight': {0: 1, 1: 60}, 'max_depth': 5...  \n",
       "1077  {'class_weight': {0: 1, 1: 90}, 'max_depth': 7...  \n",
       "1958  {'class_weight': {0: 1, 1: 60}, 'max_depth': 5...  \n",
       "362   {'class_weight': None, 'max_depth': 5, 'max_fe...  \n",
       "1748  {'class_weight': {0: 1, 1: 50}, 'max_depth': 5...  \n",
       "321   {'class_weight': {0: 1, 1: 90}, 'max_depth': 7...  \n",
       "278   {'class_weight': {0: 1, 1: 70}, 'max_depth': 5...  \n",
       "1937  {'class_weight': {0: 1, 1: 50}, 'max_depth': 5...  \n",
       "1056  {'class_weight': {0: 1, 1: 80}, 'max_depth': 7...  \n",
       "1853  {'class_weight': 'balanced', 'max_depth': 5, '...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"decision_tree_grid_search.csv\", sep=';')\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizując uzyskane wyniki można stwierdzić, że:\n",
    "\n",
    "- wśród najwyżej ocenianych nie występują zbiory przygotowane z wykorzystaniem PCA,\n",
    "- różnica pomiędzy średnim wynikiem dla zbiorów testowych i treningowych nie przekracza 3pp --> modele nie są przeuczone,\n",
    "- optymalna wartośc parametru 'class_weight' znajduje się w granicach 60-90\n",
    "- optymalna wartość parametru 'max_depth' znajduje się w granicach 5-7\n",
    "- optymalna wartość parametru 'max_features' znajduje się w granicach 75-100%\n",
    "\n",
    "Kolejnym krokiem jest modelowanie w zakresach parametrów wskazanych przez Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drzewa decyzyjne - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'f1_test', \n",
    "                   'accuracy_train', 'accuracy_test', 'precision_train', 'precision_test', 'recall_train', \n",
    "                   'recall_test', 'TN_train', 'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', \n",
    "                   'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "                             \n",
    "weights = range(50, 90, 10)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "max_depths = [5, 6, 7]\n",
    "max_features_set = [0.75, None]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in data_variants:\n",
    "        for max_depth in max_depths:\n",
    "            for max_features in max_features_set:\n",
    "                for class_weight in class_weights:\n",
    "                    X_train = datasets[dataset]['X_train' + data_variant]\n",
    "                    y_train = datasets[dataset]['y_train']\n",
    "                    X_test = datasets[dataset]['X_test' + data_variant]\n",
    "                    y_test = datasets[dataset]['y_test']               \n",
    "                    model = DecisionTreeClassifier(max_depth=max_depth, max_features=max_features, random_state=42, class_weight=class_weight)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "                    test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "                    train = train.rename(columns = train_columns_rename)\n",
    "                    test  = test.rename(columns = test_columns_rename)\n",
    "                    result = pd.concat([train, test], axis = 1)\n",
    "                    result['dataset'] = dataset\n",
    "                    result['data_variant'] = data_variant\n",
    "                    result['class_weight'] = class_weight[1]\n",
    "                    result = result[dataset_columns]\n",
    "                    model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('decision_tree_modelling.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"decision_tree_modelling.csv\", sep=';')\n",
    "results = results[results['auc_train'] - results['auc_test'] < 0.05]\n",
    "results = results.sort_values(by=['auc_train'], ascending=False)\n",
    "results = results[['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla modeli z najwyższą wartością AUC występuje znaczna różnica pomiędzy AUC dla zbioru testowego i treningowego, co wskazuje na przeuczenie modelu. Dlatego ograniczyłem analizowane wyniki wyłącznie do modeli, dla których różnica w ocenach zbioru testowego i treningowego nie przekracza 5pp AUC.\n",
    "\n",
    "Najlepszy model wykazuje 81% AUC przy bardzo słabej wartości f-score. Siła predykcyjna tego modelu jest słaba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forest\n",
    "\n",
    "Następnym klasyfikatorem użytym w modelowaniu są lasy losowe.\n",
    "Jest to naturalne rozszerzenie klasyfikacji za pomocą drzew.\n",
    "\n",
    "W optymalizacji korzystam z wypracowanego schematu postępowania, optymalizacji podlegają te same parametry, co w przypadku drzew decyzyjnych, dodatkowo optymalizuję parametr liczby estymatorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [100, 300, 400, 500, 700],\n",
       " 'max_depth': [5, 6, 7, 8, 9],\n",
       " 'max_features': [None, 0.75, 0.9],\n",
       " 'class_weight': [{0: 1, 1: 40},\n",
       "  {0: 1, 1: 60},\n",
       "  {0: 1, 1: 80},\n",
       "  'balanced',\n",
       "  None]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 100, max_depth=None, max_features=None, random_state=42, class_weight=None)\n",
    "\n",
    "n_estimators = [100, 300, 400, 500, 700]\n",
    "max_depth = [5, 6, 7, 8, 9]\n",
    "max_features = [None, 0.75, 0.90]\n",
    "weights = [40, 60, 80]\n",
    "class_weight = [{0:1, 1:x} for x in weights]\n",
    "class_weight.append('balanced')\n",
    "class_weight.append(None)\n",
    "param_grid = {'n_estimators':n_estimators, 'max_depth': max_depth, 'max_features': max_features, 'class_weight':class_weight}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_2__scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 85.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_2__woe_scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 31.8min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 85.9min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_3__scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 44.6min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 79.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 120.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_3__woe_scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 44.6min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 79.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 120.9min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_4__scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 35.0min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 62.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 94.7min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: random_forest_4__woe_scaled\n",
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 35.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 63.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed: 95.4min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        prefix = 'random_forest_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=3, n_jobs=-1, verbose = 1)\n",
    "        datasets[dataset]['grid_search_random_forest_' + data_variant] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('random_forest_grid_search.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.847175</td>\n",
       "      <td>0.912623</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.847089</td>\n",
       "      <td>0.912480</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.847078</td>\n",
       "      <td>0.912370</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846965</td>\n",
       "      <td>0.912489</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846962</td>\n",
       "      <td>0.912753</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846908</td>\n",
       "      <td>0.912417</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846857</td>\n",
       "      <td>0.912874</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846743</td>\n",
       "      <td>0.912551</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846630</td>\n",
       "      <td>0.911803</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.892889</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846530</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846458</td>\n",
       "      <td>0.912178</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846453</td>\n",
       "      <td>0.893218</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846405</td>\n",
       "      <td>0.910021</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.90</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>0.904646</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.892897</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846384</td>\n",
       "      <td>0.904581</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846363</td>\n",
       "      <td>0.892891</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846353</td>\n",
       "      <td>0.892844</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846349</td>\n",
       "      <td>0.893216</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "411  _woe_scaled        2         0.847175          0.912623   \n",
       "37       _scaled        2         0.847089          0.912480   \n",
       "36       _scaled        2         0.847078          0.912370   \n",
       "38       _scaled        2         0.846965          0.912489   \n",
       "412  _woe_scaled        2         0.846962          0.912753   \n",
       "39       _scaled        2         0.846908          0.912417   \n",
       "413  _woe_scaled        2         0.846857          0.912874   \n",
       "414  _woe_scaled        2         0.846743          0.912551   \n",
       "35       _scaled        2         0.846630          0.911803   \n",
       "396  _woe_scaled        2         0.846581          0.892889   \n",
       "112      _scaled        2         0.846530          0.904700   \n",
       "410  _woe_scaled        2         0.846458          0.912178   \n",
       "397  _woe_scaled        2         0.846453          0.893218   \n",
       "44       _scaled        2         0.846405          0.910021   \n",
       "113      _scaled        2         0.846391          0.904646   \n",
       "24       _scaled        2         0.846389          0.892897   \n",
       "111      _scaled        2         0.846384          0.904581   \n",
       "23       _scaled        2         0.846363          0.892891   \n",
       "22       _scaled        2         0.846353          0.892844   \n",
       "398  _woe_scaled        2         0.846349          0.893216   \n",
       "\n",
       "    param_class_weight  param_max_depth  param_max_features  \\\n",
       "411      {0: 1, 1: 40}                7                0.75   \n",
       "37       {0: 1, 1: 40}                7                0.75   \n",
       "36       {0: 1, 1: 40}                7                0.75   \n",
       "38       {0: 1, 1: 40}                7                0.75   \n",
       "412      {0: 1, 1: 40}                7                0.75   \n",
       "39       {0: 1, 1: 40}                7                0.75   \n",
       "413      {0: 1, 1: 40}                7                0.75   \n",
       "414      {0: 1, 1: 40}                7                0.75   \n",
       "35       {0: 1, 1: 40}                7                0.75   \n",
       "396      {0: 1, 1: 40}                6                0.75   \n",
       "112      {0: 1, 1: 60}                7                0.75   \n",
       "410      {0: 1, 1: 40}                7                0.75   \n",
       "397      {0: 1, 1: 40}                6                0.75   \n",
       "44       {0: 1, 1: 40}                7                0.90   \n",
       "113      {0: 1, 1: 60}                7                0.75   \n",
       "24       {0: 1, 1: 40}                6                0.75   \n",
       "111      {0: 1, 1: 60}                7                0.75   \n",
       "23       {0: 1, 1: 40}                6                0.75   \n",
       "22       {0: 1, 1: 40}                6                0.75   \n",
       "398      {0: 1, 1: 40}                6                0.75   \n",
       "\n",
       "     param_n_estimators                                             params  \n",
       "411                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "37                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "36                  300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "38                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "412                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "39                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "413                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "414                 700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "35                  100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "396                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "112                 400  {'class_weight': {0: 1, 1: 60}, 'max_depth': 7...  \n",
       "410                 100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "397                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "44                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 7...  \n",
       "113                 500  {'class_weight': {0: 1, 1: 60}, 'max_depth': 7...  \n",
       "24                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "111                 300  {'class_weight': {0: 1, 1: 60}, 'max_depth': 7...  \n",
       "23                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "22                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "398                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"random_forest_grid_search.csv\", sep=';')\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846581</td>\n",
       "      <td>0.892889</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846453</td>\n",
       "      <td>0.893218</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.892897</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846363</td>\n",
       "      <td>0.892891</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846353</td>\n",
       "      <td>0.892844</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846349</td>\n",
       "      <td>0.893216</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846344</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846337</td>\n",
       "      <td>0.893024</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846320</td>\n",
       "      <td>0.892534</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846244</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846142</td>\n",
       "      <td>0.891102</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846077</td>\n",
       "      <td>0.891628</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845967</td>\n",
       "      <td>0.890870</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>100</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845921</td>\n",
       "      <td>0.891595</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845839</td>\n",
       "      <td>0.891436</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845784</td>\n",
       "      <td>0.891572</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>500</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845746</td>\n",
       "      <td>0.891569</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>700</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845649</td>\n",
       "      <td>0.891602</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845612</td>\n",
       "      <td>0.891043</td>\n",
       "      <td>{0: 1, 1: 40}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 40}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.845607</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>{0: 1, 1: 60}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>400</td>\n",
       "      <td>{'class_weight': {0: 1, 1: 60}, 'max_depth': 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "396  _woe_scaled        2         0.846581          0.892889   \n",
       "397  _woe_scaled        2         0.846453          0.893218   \n",
       "24       _scaled        2         0.846389          0.892897   \n",
       "23       _scaled        2         0.846363          0.892891   \n",
       "22       _scaled        2         0.846353          0.892844   \n",
       "398  _woe_scaled        2         0.846349          0.893216   \n",
       "20       _scaled        2         0.846344          0.892500   \n",
       "399  _woe_scaled        2         0.846337          0.893024   \n",
       "21       _scaled        2         0.846320          0.892534   \n",
       "395  _woe_scaled        2         0.846244          0.892707   \n",
       "26       _scaled        2         0.846142          0.891102   \n",
       "27       _scaled        2         0.846077          0.891628   \n",
       "25       _scaled        2         0.845967          0.890870   \n",
       "28       _scaled        2         0.845921          0.891595   \n",
       "29       _scaled        2         0.845839          0.891436   \n",
       "403  _woe_scaled        2         0.845784          0.891572   \n",
       "404  _woe_scaled        2         0.845746          0.891569   \n",
       "402  _woe_scaled        2         0.845649          0.891602   \n",
       "401  _woe_scaled        2         0.845612          0.891043   \n",
       "97       _scaled        2         0.845607          0.888336   \n",
       "\n",
       "    param_class_weight  param_max_depth  param_max_features  \\\n",
       "396      {0: 1, 1: 40}                6                0.75   \n",
       "397      {0: 1, 1: 40}                6                0.75   \n",
       "24       {0: 1, 1: 40}                6                0.75   \n",
       "23       {0: 1, 1: 40}                6                0.75   \n",
       "22       {0: 1, 1: 40}                6                0.75   \n",
       "398      {0: 1, 1: 40}                6                0.75   \n",
       "20       {0: 1, 1: 40}                6                0.75   \n",
       "399      {0: 1, 1: 40}                6                0.75   \n",
       "21       {0: 1, 1: 40}                6                0.75   \n",
       "395      {0: 1, 1: 40}                6                0.75   \n",
       "26       {0: 1, 1: 40}                6                0.90   \n",
       "27       {0: 1, 1: 40}                6                0.90   \n",
       "25       {0: 1, 1: 40}                6                0.90   \n",
       "28       {0: 1, 1: 40}                6                0.90   \n",
       "29       {0: 1, 1: 40}                6                0.90   \n",
       "403      {0: 1, 1: 40}                6                0.90   \n",
       "404      {0: 1, 1: 40}                6                0.90   \n",
       "402      {0: 1, 1: 40}                6                0.90   \n",
       "401      {0: 1, 1: 40}                6                0.90   \n",
       "97       {0: 1, 1: 60}                6                0.75   \n",
       "\n",
       "     param_n_estimators                                             params  \n",
       "396                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "397                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "24                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "23                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "22                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "398                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "20                  100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "399                 700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "21                  300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "395                 100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "26                  300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "27                  400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "25                  100  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "28                  500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "29                  700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "403                 500  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "404                 700  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "402                 400  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "401                 300  {'class_weight': {0: 1, 1: 40}, 'max_depth': 6...  \n",
       "97                  400  {'class_weight': {0: 1, 1: 60}, 'max_depth': 6...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"random_forest_grid_search.csv\", sep=';')\n",
    "results = results[results['mean_train_score'] - results['mean_test_score'] < 0.05]\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki miar jakości modeli uzyskanych w Grid Search w zależności od parametrów klasyfikatora posortowałem malejąco wg średniej wartości AUC dla zbioru testowego. Analizując wyniki zwrócone dla najlepszych 20 szacowań (powyżej) można stwierdzić, że: \n",
    "- róznica na AUC pomiędzy zbiorem testowym i treningowym przekracza 5pp, co może wskazywać na przeuczenie modeli,\n",
    "- występuje wyłącznie dataset 2,\n",
    "- jako waga dla klasy objaśnianej występuje prawie wyłącznie wartość 40, \n",
    "- parametr max_depth występuje w granicach 6-7,\n",
    "- parametr max_features występuje wyłącznie w wartości 0.75\n",
    "- dominuje 'class_weight' na poziomie 40\n",
    "- przeważa ;max_weight' na poziomie 75%,\n",
    "- jakość modelu jest nieczuła na liczbę klasyfikatorów w badanym zakresie\n",
    "\n",
    "Po wprowadzeniu ograniczenia w zbiorze wyników - dopuszczalna różnica AUC_train - AUC_test < 5% (wykluczenie modeli przeuczonych; wyniki poniżej):\n",
    "- wcześniejsze obserwacje pozostają w mocy.\n",
    "\n",
    "Dziwi brak czułości metody na parametr liczby estymatorów - prawdopodobnie przeszukiwany zestaw zawierał zbyt wysokie wartości. Podobnie wskazana wartość 'class_weight' to najniższa wartość w badanym zestawie. Niestety ze wzgledu na długi czas obliczeń nie byłem w stanie przetworzyć ponownie Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W etapie modelowania generuję zestaw modeli z parametrami klasyfikatora w granicach wskazanych jako najlepsze w Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'n_estimators', 'max_depth', 'max_features', 'class_weight', \n",
    "                   'auc_train', 'auc_test', 'f1_train', 'f1_test', 'accuracy_train', 'accuracy_test', \n",
    "                   'precision_train', 'precision_test', 'recall_train', 'recall_test', 'TN_train', \n",
    "                   'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "\n",
    "n_estimators_set = [10, 50, 100, 300] \n",
    "weights = range(20, 60, 5)\n",
    "class_weights = [{0:1, 1:x} for x in weights]\n",
    "max_depths = [5, 6, 7, 8]\n",
    "max_features_set = [0.65, 0.75, 0.85]\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        for n_estimators in n_estimators_set:\n",
    "            for max_depth in max_depths:\n",
    "                for max_features in max_features_set:\n",
    "                    for class_weight in class_weights:\n",
    "                        X_train = datasets[dataset]['X_train' + data_variant]\n",
    "                        y_train = datasets[dataset]['y_train']\n",
    "                        X_test = datasets[dataset]['X_test' + data_variant]\n",
    "                        y_test = datasets[dataset]['y_test']               \n",
    "                        model = RandomForestClassifier(n_estimators = n_estimators, max_depth=max_depth, max_features=max_features, random_state=42, class_weight=class_weight, n_jobs=-1)\n",
    "                        model.fit(X_train, y_train)\n",
    "                        train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "                        test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "                        train = train.rename(columns = train_columns_rename)\n",
    "                        test  = test.rename(columns = test_columns_rename)\n",
    "                        result = pd.concat([train, test], axis = 1)\n",
    "                        result['dataset'] = dataset\n",
    "                        result['data_variant'] = data_variant\n",
    "                        result['n_estimators'] = n_estimators\n",
    "                        result['max_depth'] = max_depth\n",
    "                        result['max_features'] = max_features\n",
    "                        result['class_weight'] = class_weight[1]\n",
    "                        result = result[dataset_columns]\n",
    "                        model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('random_forest_modelling.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_variant</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>auc_train</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>4</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>60</td>\n",
       "      <td>0.809450</td>\n",
       "      <td>0.760044</td>\n",
       "      <td>0.083704</td>\n",
       "      <td>0.706811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>60</td>\n",
       "      <td>0.805169</td>\n",
       "      <td>0.761743</td>\n",
       "      <td>0.078568</td>\n",
       "      <td>0.679062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>60</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.761580</td>\n",
       "      <td>0.078471</td>\n",
       "      <td>0.678633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.804064</td>\n",
       "      <td>0.754341</td>\n",
       "      <td>0.083418</td>\n",
       "      <td>0.710427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>4</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>50</td>\n",
       "      <td>0.804004</td>\n",
       "      <td>0.759423</td>\n",
       "      <td>0.092379</td>\n",
       "      <td>0.754018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>50</td>\n",
       "      <td>0.803943</td>\n",
       "      <td>0.760031</td>\n",
       "      <td>0.088238</td>\n",
       "      <td>0.735601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>50</td>\n",
       "      <td>0.802554</td>\n",
       "      <td>0.754199</td>\n",
       "      <td>0.088472</td>\n",
       "      <td>0.737945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>60</td>\n",
       "      <td>0.802378</td>\n",
       "      <td>0.763759</td>\n",
       "      <td>0.078662</td>\n",
       "      <td>0.682709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>70</td>\n",
       "      <td>0.802095</td>\n",
       "      <td>0.761473</td>\n",
       "      <td>0.086938</td>\n",
       "      <td>0.730943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>50</td>\n",
       "      <td>0.800760</td>\n",
       "      <td>0.762643</td>\n",
       "      <td>0.088462</td>\n",
       "      <td>0.739493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset data_variant  class_weight  auc_train  auc_test  f1_train  \\\n",
       "330        4      _scaled            60   0.809450  0.760044  0.083704   \n",
       "163        2  _woe_scaled            60   0.805169  0.761743  0.078568   \n",
       "139        2      _scaled            60   0.804952  0.761580  0.078471   \n",
       "164        2  _woe_scaled            70   0.804064  0.754341  0.083418   \n",
       "328        4      _scaled            50   0.804004  0.759423  0.092379   \n",
       "137        2      _scaled            50   0.803943  0.760031  0.088238   \n",
       "257        3  _woe_scaled            50   0.802554  0.754199  0.088472   \n",
       "259        3  _woe_scaled            60   0.802378  0.763759  0.078662   \n",
       "165        2  _woe_scaled            70   0.802095  0.761473  0.086938   \n",
       "161        2  _woe_scaled            50   0.800760  0.762643  0.088462   \n",
       "\n",
       "     accuracy_train  \n",
       "330        0.706811  \n",
       "163        0.679062  \n",
       "139        0.678633  \n",
       "164        0.710427  \n",
       "328        0.754018  \n",
       "137        0.735601  \n",
       "257        0.737945  \n",
       "259        0.682709  \n",
       "165        0.730943  \n",
       "161        0.739493  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"decision_tree_modelling.csv\", sep=';')\n",
    "results = results[results['auc_train'] - results['auc_test'] < 0.05]\n",
    "results = results.sort_values(by=['auc_train'], ascending=False)\n",
    "results = results[['dataset', 'data_variant', 'class_weight', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyżej ocena jakości uzyskanych modeli - posortowana malejąco wg AUC na zbiorze treningowym. Usunąłem modele, dla których różnica AUC pomiędzy zbiorem testowym i treniongowym przekraczała 5pp (przeuczenie).\n",
    "Najlepsze rezultaty otrzymano dla datasetów 2 i 4 przy przeważeniu klas dla zmiennej objaśnianej na poziomie ok 60. Podobnie jak dla poprzednio użytych klasyfikatorów jakość najlepszych modeli nie jest zbyt dobra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gradient Boosting\n",
    "\n",
    "Kolejnym z badanych klasyfikatorów jest Gradient Boosting. Pierwszym krokiem jest Grid Search, optymalizacji poddaję następujące parametry:\n",
    "\n",
    "- learning_rate,\n",
    "- liczba estymatorów\n",
    "- maksymalną głębokość drzewa,\n",
    "- maksymalną ilość zmiennych uwzględnianych w danym kroku budowania drzewa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.01, 0.05, 0.1],\n",
       " 'n_estimators': [20, 50, 100, 200, 500],\n",
       " 'max_depth': [3, 5, 7],\n",
       " 'max_features': [None, 0.5, 0.75, 0.9]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, subsample=1., max_depth=5, max_features=None, \n",
    "                                        random_state=42, verbose=1)\n",
    "    \n",
    "learning_rate = [0.01, 0.05, 0.1]\n",
    "n_estimators = [20, 50, 100, 200, 500]\n",
    "max_depth = [3, 5, 7]\n",
    "max_features = [None, 0.5, 0.75, 0.9]                            \n",
    "param_grid = {'learning_rate': learning_rate, 'n_estimators': n_estimators, 'max_depth':max_depth, 'max_features':max_features}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: gradient_boosting_3__pca\n",
      "Fitting 4 folds for each of 180 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 36.2min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 67.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.1513            1.70m\n",
      "         2           0.1500            1.71m\n",
      "         3           0.1490            1.66m\n",
      "         4           0.1478            1.69m\n",
      "         5           0.1469            1.68m\n",
      "         6           0.1460            1.68m\n",
      "         7           0.1451            1.69m\n",
      "         8           0.1441            1.70m\n",
      "         9           0.1433            1.71m\n",
      "        10           0.1425            1.71m\n",
      "        20           0.1356            1.69m\n",
      "        30           0.1299            1.66m\n",
      "        40           0.1252            1.63m\n",
      "        50           0.1214            1.59m\n",
      "        60           0.1182            1.57m\n",
      "        70           0.1151            1.52m\n",
      "        80           0.1129            1.48m\n",
      "        90           0.1106            1.45m\n",
      "       100           0.1089            1.40m\n",
      "       200           0.0941            1.03m\n",
      "       300           0.0863           40.54s\n",
      "       400           0.0826           19.74s\n",
      "       500           0.0794            0.00s\n",
      "start: gradient_boosting_3__scaled\n",
      "Fitting 4 folds for each of 180 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-95c6e302d870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_variant\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mgrid_search\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_grid_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'grid_search_gradient_boosting_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_variant\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_variant'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_variant\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-285d2462553d>\u001b[0m in \u001b[0;36mmodel_grid_search\u001b[1;34m(prefix, classifier, param_grid, X, y, scoring, cv, n_jobs, verbose)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_grid_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_grid_search.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in [3, 4]:\n",
    "    for data_variant in ['_pca', '_scaled', '_woe_scaled', '_woe_pca']:\n",
    "        prefix = 'gradient_boosting_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=4, n_jobs=-1, verbose = 1)\n",
    "        datasets[dataset]['grid_search_gradient_boosting_' + data_variant] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('gradient_boosting_grid_search.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_variant</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.849107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.917349</td>\n",
       "      <td>0.849024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.929351</td>\n",
       "      <td>0.848823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.910566</td>\n",
       "      <td>0.848539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.928798</td>\n",
       "      <td>0.848518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.911476</td>\n",
       "      <td>0.848180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.909537</td>\n",
       "      <td>0.848110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.918456</td>\n",
       "      <td>0.848103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.926236</td>\n",
       "      <td>0.848100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.918972</td>\n",
       "      <td>0.848044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.908824</td>\n",
       "      <td>0.847825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.918640</td>\n",
       "      <td>0.847801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.909717</td>\n",
       "      <td>0.847762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.911403</td>\n",
       "      <td>0.847755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.889047</td>\n",
       "      <td>0.847712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.918442</td>\n",
       "      <td>0.847685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.917032</td>\n",
       "      <td>0.847629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.918208</td>\n",
       "      <td>0.847596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.911119</td>\n",
       "      <td>0.847555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.889684</td>\n",
       "      <td>0.847550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset data_variant  param_learning_rate  param_n_estimators  \\\n",
       "69         2      _scaled                 0.05                 500   \n",
       "439        2  _woe_scaled                 0.05                 500   \n",
       "29         2      _scaled                 0.01                 500   \n",
       "498        2  _woe_scaled                 0.10                 200   \n",
       "389        2  _woe_scaled                 0.01                 500   \n",
       "138        2      _scaled                 0.10                 200   \n",
       "128        2      _scaled                 0.10                 200   \n",
       "74         2      _scaled                 0.05                 500   \n",
       "394        2  _woe_scaled                 0.01                 500   \n",
       "434        2  _woe_scaled                 0.05                 500   \n",
       "488        2  _woe_scaled                 0.10                 200   \n",
       "64         2      _scaled                 0.05                 500   \n",
       "133        2      _scaled                 0.10                 200   \n",
       "123        2      _scaled                 0.10                 200   \n",
       "487        2  _woe_scaled                 0.10                 100   \n",
       "424        2  _woe_scaled                 0.05                 500   \n",
       "429        2  _woe_scaled                 0.05                 500   \n",
       "79         2      _scaled                 0.05                 500   \n",
       "483        2  _woe_scaled                 0.10                 200   \n",
       "68         2      _scaled                 0.05                 200   \n",
       "\n",
       "     param_max_depth  param_max_features  mean_train_score  mean_test_score  \n",
       "69                 3                0.50          0.916667         0.849107  \n",
       "439                3                0.90          0.917349         0.849024  \n",
       "29                 5                0.50          0.929351         0.848823  \n",
       "498                3                0.90          0.910566         0.848539  \n",
       "389                5                0.50          0.928798         0.848518  \n",
       "138                3                0.90          0.911476         0.848180  \n",
       "128                3                0.50          0.909537         0.848110  \n",
       "74                 3                0.75          0.918456         0.848103  \n",
       "394                5                0.75          0.926236         0.848100  \n",
       "434                3                0.75          0.918972         0.848044  \n",
       "488                3                0.50          0.908824         0.847825  \n",
       "64                 3                 NaN          0.918640         0.847801  \n",
       "133                3                0.75          0.909717         0.847762  \n",
       "123                3                 NaN          0.911403         0.847755  \n",
       "487                3                0.50          0.889047         0.847712  \n",
       "424                3                 NaN          0.918442         0.847685  \n",
       "429                3                0.50          0.917032         0.847629  \n",
       "79                 3                0.90          0.918208         0.847596  \n",
       "483                3                 NaN          0.911119         0.847555  \n",
       "68                 3                0.50          0.889684         0.847550  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"gradient_boosting_grid_search.csv\", sep=';')\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedura nie została zakończona z uwagi na dość długi czas oblcizeń.\n",
    "W oparciu o wyniki uzyskane z częściowego przetworzenia można wyciągnąć następujace wnioski:\n",
    "- lekkie przeuczenie modeli (różnica AUC train-test w granicach 6-7 pp)\n",
    "- lepsze wyniki niż dla pozostałych klasyfikatorów,\n",
    "- 'learning_rate' zróżnicowany w top 20 --> niewielki wpływ na jakość\n",
    "- 'n_estimators' - dominuje 500 i 200\n",
    "- 'max_depth; - dominuje 3 i 5 (\"weak learners\")\n",
    "- 'max_features' - zróżnicowany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_variant</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.889047</td>\n",
       "      <td>0.847712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.889684</td>\n",
       "      <td>0.847550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.888654</td>\n",
       "      <td>0.847411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.891054</td>\n",
       "      <td>0.847363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.890735</td>\n",
       "      <td>0.847318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.890451</td>\n",
       "      <td>0.847225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.889247</td>\n",
       "      <td>0.847031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.891683</td>\n",
       "      <td>0.846907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.891487</td>\n",
       "      <td>0.846885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.890987</td>\n",
       "      <td>0.846856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891211</td>\n",
       "      <td>0.846803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.891412</td>\n",
       "      <td>0.846700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.890868</td>\n",
       "      <td>0.846398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.890737</td>\n",
       "      <td>0.846295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.891179</td>\n",
       "      <td>0.846293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.890259</td>\n",
       "      <td>0.846047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.874650</td>\n",
       "      <td>0.843958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.874898</td>\n",
       "      <td>0.843728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.874656</td>\n",
       "      <td>0.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.874659</td>\n",
       "      <td>0.843591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset data_variant  param_learning_rate  param_n_estimators  \\\n",
       "487        2  _woe_scaled                 0.10                 100   \n",
       "68         2      _scaled                 0.05                 200   \n",
       "127        2      _scaled                 0.10                 100   \n",
       "433        2  _woe_scaled                 0.05                 200   \n",
       "438        2  _woe_scaled                 0.05                 200   \n",
       "137        2      _scaled                 0.10                 100   \n",
       "428        2  _woe_scaled                 0.05                 200   \n",
       "78         2      _scaled                 0.05                 200   \n",
       "73         2      _scaled                 0.05                 200   \n",
       "423        2  _woe_scaled                 0.05                 200   \n",
       "63         2      _scaled                 0.05                 200   \n",
       "132        2      _scaled                 0.10                 100   \n",
       "497        2  _woe_scaled                 0.10                 100   \n",
       "122        2      _scaled                 0.10                 100   \n",
       "482        2  _woe_scaled                 0.10                 100   \n",
       "492        2  _woe_scaled                 0.10                 100   \n",
       "9          2      _scaled                 0.01                 500   \n",
       "437        2  _woe_scaled                 0.05                 100   \n",
       "374        2  _woe_scaled                 0.01                 500   \n",
       "72         2      _scaled                 0.05                 100   \n",
       "\n",
       "     param_max_depth  param_max_features  mean_train_score  mean_test_score  \n",
       "487                3                0.50          0.889047         0.847712  \n",
       "68                 3                0.50          0.889684         0.847550  \n",
       "127                3                0.50          0.888654         0.847411  \n",
       "433                3                0.75          0.891054         0.847363  \n",
       "438                3                0.90          0.890735         0.847318  \n",
       "137                3                0.90          0.890451         0.847225  \n",
       "428                3                0.50          0.889247         0.847031  \n",
       "78                 3                0.90          0.891683         0.846907  \n",
       "73                 3                0.75          0.891487         0.846885  \n",
       "423                3                 NaN          0.890987         0.846856  \n",
       "63                 3                 NaN          0.891211         0.846803  \n",
       "132                3                0.75          0.891412         0.846700  \n",
       "497                3                0.90          0.890868         0.846398  \n",
       "122                3                 NaN          0.890737         0.846295  \n",
       "482                3                 NaN          0.891179         0.846293  \n",
       "492                3                0.75          0.890259         0.846047  \n",
       "9                  3                0.50          0.874650         0.843958  \n",
       "437                3                0.90          0.874898         0.843728  \n",
       "374                3                0.75          0.874656         0.843700  \n",
       "72                 3                0.75          0.874659         0.843591  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"gradient_boosting_grid_search.csv\", sep=';')\n",
    "results = results[results['mean_train_score'] - results['mean_test_score'] < 0.05]\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po wyeliminowaniu modeli przeuczonych ( > 5pp różnicy na AUC pomiędzy train-test) - w oparciu o prezentowane wyniki można podtrzymać wysnute wcześniej wnioski.\n",
    "\n",
    "Wyniki częsciowego przebiegu Grid Search są wystarczające, zeby przejść do fazy wyznaczania modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.75\n",
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.9\n",
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.5\n",
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.75\n",
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.9\n",
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.5\n",
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.75\n",
      "dataset: 2, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.9\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.5\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.75\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.9\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.5\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.75\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.9\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.5\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.75\n",
      "dataset: 2, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.9\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.5\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.75\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.9\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.5\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.75\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.9\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.5\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.75\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.9\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.5\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.75\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.9\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.5\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.75\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.9\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.5\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.75\n",
      "dataset: 2, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.9\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.5\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.75\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.9\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.5\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.75\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.9\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.5\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.75\n",
      "dataset: 3, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.9\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.5\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.75\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.9\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.5\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.75\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.9\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.5\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.75\n",
      "dataset: 3, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.9\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.5\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.75\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.9\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.5\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.75\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.9\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.5\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.75\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.9\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.5\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.75\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.9\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.5\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.75\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.9\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.5\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.75\n",
      "dataset: 3, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.9\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.5\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.75\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 3,max_features: 0.9\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.5\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.75\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 4,max_features: 0.9\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.5\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.75\n",
      "dataset: 4, data variant: _scaled, n_estimators: 100, max_depth: 5,max_features: 0.9\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.5\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.75\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 3,max_features: 0.9\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.5\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.75\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 4,max_features: 0.9\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.5\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.75\n",
      "dataset: 4, data variant: _scaled, n_estimators: 300, max_depth: 5,max_features: 0.9\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.5\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.75\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 3,max_features: 0.9\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.5\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.75\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 4,max_features: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.5\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.75\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 100, max_depth: 5,max_features: 0.9\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.5\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.75\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 3,max_features: 0.9\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.5\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.75\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 4,max_features: 0.9\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.5\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.75\n",
      "dataset: 4, data variant: _woe_scaled, n_estimators: 300, max_depth: 5,max_features: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'n_estimators', 'max_depth', 'max_features',  \n",
    "                   'auc_train', 'auc_test', 'f1_train', 'f1_test', 'accuracy_train', 'accuracy_test', \n",
    "                   'precision_train', 'precision_test', 'recall_train', 'recall_test', 'TN_train', \n",
    "                   'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "\n",
    "n_estimators_set = [100, 300] \n",
    "max_depths = [3, 4, 5]\n",
    "max_features_set = [0.5, 0.75, 0.9]\n",
    "\n",
    "\n",
    "for dataset in [2, 3, 4]:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        for n_estimators in n_estimators_set:\n",
    "            for max_depth in max_depths:\n",
    "                for max_features in max_features_set:\n",
    "                    print(f\"dataset: {dataset}, data variant: {data_variant}, n_estimators: {n_estimators}, max_depth: {max_depth},max_features: {max_features}\")\n",
    "                    X_train = datasets[dataset]['X_train' + data_variant]\n",
    "                    y_train = datasets[dataset]['y_train']\n",
    "                    X_test = datasets[dataset]['X_test' + data_variant]\n",
    "                    y_test = datasets[dataset]['y_test']               \n",
    "                    model = GradientBoostingClassifier(learning_rate=0.01, n_estimators=n_estimators, max_depth=max_depth, subsample = 1.,\n",
    "                                                       max_features=max_features, random_state=42, verbose=0)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "                    test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "                    train = train.rename(columns = train_columns_rename)\n",
    "                    test  = test.rename(columns = test_columns_rename)\n",
    "                    result = pd.concat([train, test], axis = 1)\n",
    "                    result['dataset'] = dataset\n",
    "                    result['data_variant'] = data_variant\n",
    "                    result['n_estimators'] = n_estimators\n",
    "                    result['max_depth'] = max_depth\n",
    "                    result['max_features'] = max_features\n",
    "                    result = result[dataset_columns]\n",
    "                    model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('gradient_boosting_modelling.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_variant</th>\n",
       "      <th>auc_train</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.503141</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.985459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.503141</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.985459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.503141</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.985459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.502618</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.985444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.502618</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.985444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.502618</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.985444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.502618</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.985444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.502094</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008342</td>\n",
       "      <td>0.985429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.502094</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008342</td>\n",
       "      <td>0.985429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.502094</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008342</td>\n",
       "      <td>0.985429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset data_variant  auc_train  auc_test  f1_train  accuracy_train\n",
       "35         2  _woe_scaled   0.503141  0.499977  0.012487        0.985459\n",
       "70         3  _woe_scaled   0.503141  0.499977  0.012487        0.985459\n",
       "16         2      _scaled   0.503141  0.499977  0.012487        0.985459\n",
       "15         2      _scaled   0.502618  0.500000  0.010417        0.985444\n",
       "34         2  _woe_scaled   0.502618  0.499977  0.010417        0.985444\n",
       "71         3  _woe_scaled   0.502618  0.499977  0.010417        0.985444\n",
       "17         2      _scaled   0.502618  0.499977  0.010417        0.985444\n",
       "106        4  _woe_scaled   0.502094  0.500000  0.008342        0.985429\n",
       "105        4  _woe_scaled   0.502094  0.500000  0.008342        0.985429\n",
       "33         2  _woe_scaled   0.502094  0.500000  0.008342        0.985429"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"gradient_boosting_modelling.csv\", sep=';')\n",
    "# results = results[results['auc_train'] - results['auc_test'] < 0.05]\n",
    "results = results.sort_values(by=['auc_train'], ascending=False)\n",
    "results = results[['dataset', 'data_variant', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoda skupia się wyłącznie na optymalizacji accuracy, co przy niezrównoważonej próbce prowadzi do 'zdegenerowanego' modelu, który w zasadzie przypisuje wszystkim rekordom zerową zmienną obserwowaną.\n",
    "\n",
    "Na etapie Grid Searcha optymalizowałem AUC, sam klasyfikator nie daje możliwości wyboru optymalizowanej miary.\n",
    "Nie ma też mozliwości w samym klasyfikatorze przeważenia klas w zmiennej objaśnianej, co zaskutkowało 'klęską' w poszukiwaniu optymalnego modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. XGBoost\n",
    "\n",
    "Ostatnim wypróbowanym klasyfikatorem jest XGBoost.\n",
    "Przebieg optymalizacji - jak dla pozostałych klasyfikatorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.01, 0.1],\n",
       " 'n_estimators': [200, 400, 600],\n",
       " 'max_depth': [3, 5, 7],\n",
       " 'scale_pos_weight': [20, 40, 60]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = xgboost.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, verbosity=1, scale_pos_weight=1)\n",
    "    \n",
    "learning_rate = [0.01, 0.1]\n",
    "n_estimators = [200, 400, 600]\n",
    "max_depth = [3, 5, 7]\n",
    "scale_pos_weight = [20, 40, 60]                            \n",
    "param_grid = {'learning_rate': learning_rate, 'n_estimators': n_estimators, 'max_depth':max_depth, 'scale_pos_weight':scale_pos_weight}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['dataset', 'data_variant', 'params', 'mean_train_score', 'mean_test_score'])\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        prefix = 'xgboost_' + str(dataset) + '_' + data_variant\n",
    "        print(\"start: \" + prefix)\n",
    "        X = datasets[dataset]['X_train' + data_variant]\n",
    "        y = datasets[dataset]['y_train']\n",
    "        grid_search, result = model_grid_search(prefix, classifier, param_grid, X, y, scoring='roc_auc', cv=4, n_jobs=-1, verbose = 1)\n",
    "        datasets[dataset]['grid_search_xgboost_' + data_variant] = grid_search\n",
    "        result['dataset'], result['data_variant'] = str(dataset), data_variant\n",
    "        results = pd.concat([results, result])\n",
    "\n",
    "results.to_csv('xgboost_grid_search.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.853271</td>\n",
       "      <td>0.926956</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>0.914178</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.852185</td>\n",
       "      <td>0.916303</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.852183</td>\n",
       "      <td>0.922133</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851975</td>\n",
       "      <td>0.921423</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851975</td>\n",
       "      <td>0.913451</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.851967</td>\n",
       "      <td>0.919583</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851927</td>\n",
       "      <td>0.914013</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851908</td>\n",
       "      <td>0.913781</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.851880</td>\n",
       "      <td>0.918689</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.851834</td>\n",
       "      <td>0.927776</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851680</td>\n",
       "      <td>0.915676</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851678</td>\n",
       "      <td>0.907158</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851550</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851510</td>\n",
       "      <td>0.877264</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851401</td>\n",
       "      <td>0.915853</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851333</td>\n",
       "      <td>0.878806</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851313</td>\n",
       "      <td>0.874652</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851297</td>\n",
       "      <td>0.902928</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.851296</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "0   _woe_scaled        3         0.853271          0.926956   \n",
       "1   _woe_scaled        2         0.852590          0.914178   \n",
       "2   _woe_scaled        2         0.852185          0.916303   \n",
       "3   _woe_scaled        2         0.852183          0.922133   \n",
       "4       _scaled        2         0.851975          0.921423   \n",
       "5       _scaled        2         0.851975          0.913451   \n",
       "6   _woe_scaled        3         0.851967          0.919583   \n",
       "7       _scaled        1         0.851927          0.914013   \n",
       "8   _woe_scaled        1         0.851908          0.913781   \n",
       "9       _scaled        3         0.851880          0.918689   \n",
       "10      _scaled        3         0.851834          0.927776   \n",
       "11      _scaled        2         0.851680          0.915676   \n",
       "12      _scaled        1         0.851678          0.907158   \n",
       "13  _woe_scaled        2         0.851550          0.901090   \n",
       "14      _scaled        2         0.851510          0.877264   \n",
       "15  _woe_scaled        1         0.851401          0.915853   \n",
       "16      _scaled        2         0.851333          0.878806   \n",
       "17  _woe_scaled        2         0.851313          0.874652   \n",
       "18  _woe_scaled        2         0.851297          0.902928   \n",
       "19  _woe_scaled        3         0.851296          0.929369   \n",
       "\n",
       "    param_learning_rate  param_max_depth  param_n_estimators  \\\n",
       "0                  0.10                3                 200   \n",
       "1                  0.01                5                 600   \n",
       "2                  0.01                5                 600   \n",
       "3                  0.10                3                 200   \n",
       "4                  0.10                3                 200   \n",
       "5                  0.01                5                 600   \n",
       "6                  0.01                5                 600   \n",
       "7                  0.10                3                 200   \n",
       "8                  0.10                3                 200   \n",
       "9                  0.01                5                 600   \n",
       "10                 0.10                3                 200   \n",
       "11                 0.01                5                 600   \n",
       "12                 0.01                5                 600   \n",
       "13                 0.01                5                 400   \n",
       "14                 0.01                3                 600   \n",
       "15                 0.10                3                 200   \n",
       "16                 0.01                3                 600   \n",
       "17                 0.01                3                 600   \n",
       "18                 0.01                5                 400   \n",
       "19                 0.10                3                 200   \n",
       "\n",
       "    param_scale_pos_weight  \n",
       "0                       20  \n",
       "1                       20  \n",
       "2                       40  \n",
       "3                       20  \n",
       "4                       20  \n",
       "5                       20  \n",
       "6                       20  \n",
       "7                       20  \n",
       "8                       20  \n",
       "9                       20  \n",
       "10                      20  \n",
       "11                      40  \n",
       "12                      20  \n",
       "13                      20  \n",
       "14                      40  \n",
       "15                      40  \n",
       "16                      60  \n",
       "17                      20  \n",
       "18                      40  \n",
       "19                      40  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"xgboost_grid_search.csv\", sep=';')\n",
    "results = results.drop(['params'], axis=1)\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wnioski:\n",
    "- lekkie przetrenowanie modeli,\n",
    "- wartości AUC porównywalne z otrzymanymi dla Gradient Boostingu\n",
    "- learning_rate nie wpływa znacząco na jakość modelu\n",
    "- max depth na poziomie 3-5 (\"weak learner\")\n",
    "- liczba estymatorów - preferowana wysoka\n",
    "- parametr skalujacy zmienną obserwowaną - na poziomie ok. 20.\n",
    "\n",
    "Poniżej ten sam zestaw z ograniczeniem na AUC_train - AUC_test < 5%.\n",
    "Analiza tego zestawienia wnosi nowe informacje:\n",
    "- learning rate - preferowany poziom 0.01,\n",
    "- scale_pos_weight - bez większego wpływu na model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_variant</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851550</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851510</td>\n",
       "      <td>0.877264</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851333</td>\n",
       "      <td>0.878806</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851313</td>\n",
       "      <td>0.874652</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851285</td>\n",
       "      <td>0.874665</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851274</td>\n",
       "      <td>0.877272</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851085</td>\n",
       "      <td>0.878750</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850593</td>\n",
       "      <td>0.895698</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850402</td>\n",
       "      <td>0.895804</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850399</td>\n",
       "      <td>0.875355</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850377</td>\n",
       "      <td>0.877755</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850366</td>\n",
       "      <td>0.875516</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850322</td>\n",
       "      <td>0.877762</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850266</td>\n",
       "      <td>0.879512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850218</td>\n",
       "      <td>0.879290</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850137</td>\n",
       "      <td>0.895479</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.849598</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.849203</td>\n",
       "      <td>0.897501</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.873838</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>_scaled</td>\n",
       "      <td>4</td>\n",
       "      <td>0.848729</td>\n",
       "      <td>0.874418</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_variant  dataset  mean_test_score  mean_train_score  \\\n",
       "13  _woe_scaled        2         0.851550          0.901090   \n",
       "14      _scaled        2         0.851510          0.877264   \n",
       "16      _scaled        2         0.851333          0.878806   \n",
       "17  _woe_scaled        2         0.851313          0.874652   \n",
       "20      _scaled        2         0.851285          0.874665   \n",
       "21  _woe_scaled        2         0.851274          0.877272   \n",
       "23  _woe_scaled        2         0.851085          0.878750   \n",
       "37      _scaled        1         0.850593          0.895698   \n",
       "40  _woe_scaled        1         0.850402          0.895804   \n",
       "41      _scaled        3         0.850399          0.875355   \n",
       "42      _scaled        3         0.850377          0.877755   \n",
       "43  _woe_scaled        3         0.850366          0.875516   \n",
       "45  _woe_scaled        3         0.850322          0.877762   \n",
       "47  _woe_scaled        3         0.850266          0.879512   \n",
       "48      _scaled        3         0.850218          0.879290   \n",
       "52      _scaled        1         0.850137          0.895479   \n",
       "63  _woe_scaled        1         0.849598          0.895532   \n",
       "67      _scaled        1         0.849203          0.897501   \n",
       "76  _woe_scaled        1         0.848739          0.873838   \n",
       "77      _scaled        4         0.848729          0.874418   \n",
       "\n",
       "    param_learning_rate  param_max_depth  param_n_estimators  \\\n",
       "13                 0.01                5                 400   \n",
       "14                 0.01                3                 600   \n",
       "16                 0.01                3                 600   \n",
       "17                 0.01                3                 600   \n",
       "20                 0.01                3                 600   \n",
       "21                 0.01                3                 600   \n",
       "23                 0.01                3                 600   \n",
       "37                 0.01                5                 400   \n",
       "40                 0.01                5                 400   \n",
       "41                 0.01                3                 600   \n",
       "42                 0.01                3                 600   \n",
       "43                 0.01                3                 600   \n",
       "45                 0.01                3                 600   \n",
       "47                 0.01                3                 600   \n",
       "48                 0.01                3                 600   \n",
       "52                 0.01                5                 400   \n",
       "63                 0.01                5                 400   \n",
       "67                 0.01                5                 400   \n",
       "76                 0.01                3                 600   \n",
       "77                 0.01                3                 600   \n",
       "\n",
       "    param_scale_pos_weight  \n",
       "13                      20  \n",
       "14                      40  \n",
       "16                      60  \n",
       "17                      20  \n",
       "20                      20  \n",
       "21                      40  \n",
       "23                      60  \n",
       "37                      20  \n",
       "40                      40  \n",
       "41                      20  \n",
       "42                      40  \n",
       "43                      20  \n",
       "45                      40  \n",
       "47                      60  \n",
       "48                      60  \n",
       "52                      40  \n",
       "63                      20  \n",
       "67                      60  \n",
       "76                      40  \n",
       "77                      20  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"xgboost_grid_search.csv\", sep=';')\n",
    "results = results.drop(['params'], axis=1)\n",
    "results = results[results['mean_train_score'] - results['mean_test_score'] < 0.05]\n",
    "results = results.sort_values(by=['mean_test_score'], ascending=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 1 data variant _scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 1 data variant _woe_scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 2 data variant _scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 2 data variant _woe_scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 3 data variant _scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 3 data variant _woe_scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 4 data variant _scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: dataset 4 data variant _woe_scaled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# XGBoost - modelowanie\n",
    "dataset_columns = ['dataset', 'data_variant', 'n_estimators', 'max_depth',   \n",
    "                   'auc_train', 'auc_test', 'f1_train', 'f1_test', 'accuracy_train', 'accuracy_test', \n",
    "                   'precision_train', 'precision_test', 'recall_train', 'recall_test', 'TN_train', \n",
    "                   'TN_test', 'FN_train', 'FN_test', 'FP_train',  'FP_test', 'TP_train', 'TP_test']\n",
    "\n",
    "model_results = pd.DataFrame(columns = dataset_columns)\n",
    "\n",
    "train_columns_rename = {'accuracy':'accuracy_train', 'precision':'precision_train', 'recall':'recall_train', \n",
    "                        'f1':'f1_train', 'auc':'auc_train', 'TP':'TP_train', 'FP':'FP_train', 'TN':'TN_train', \n",
    "                        'FN':'FN_train'}\n",
    "\n",
    "test_columns_rename = {'accuracy':'accuracy_test', 'precision':'precision_test', 'recall':'recall_test', \n",
    "                       'f1':'f1_test', 'auc':'auc_test', 'TP':'TP_test', 'FP':'FP_test', 'TN':'TN_test', \n",
    "                       'FN':'FN_test'}\n",
    "\n",
    "n_estimators_set = [400, 500, 600] \n",
    "max_depths = [3, 4, 5]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_variant in ['_scaled', '_woe_scaled']:\n",
    "        print(f\"start: dataset {dataset} data variant {data_variant}\")\n",
    "        for n_estimators in n_estimators_set:\n",
    "            for max_depth in max_depths:\n",
    "                X_train = datasets[dataset]['X_train' + data_variant]\n",
    "                y_train = datasets[dataset]['y_train']\n",
    "                X_test = datasets[dataset]['X_test' + data_variant]\n",
    "                y_test = datasets[dataset]['y_test']               \n",
    "                model = xgboost.XGBClassifier(n_jobs = 10, max_depth=max_depth, learning_rate=0.01, n_estimators=n_estimators, verbosity=0, scale_pos_weight=50)\n",
    "                model.fit(X_train, y_train)\n",
    "                train = pd.DataFrame.from_dict(get_measures(model, X_train, y_train), orient = 'index').transpose()\n",
    "                test  = pd.DataFrame.from_dict(get_measures(model, X_test , y_test ), orient = 'index').transpose()\n",
    "                train = train.rename(columns = train_columns_rename)\n",
    "                test  = test.rename(columns = test_columns_rename)\n",
    "                result = pd.concat([train, test], axis = 1)\n",
    "                result['dataset'] = dataset\n",
    "                result['data_variant'] = data_variant\n",
    "                result['n_estimators'] = n_estimators\n",
    "                result['max_depth'] = max_depth\n",
    "                result = result[dataset_columns]\n",
    "                model_results = pd.concat([model_results, result])\n",
    "\n",
    "model_results.to_csv('xgboost_modelling.csv', index=False, sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_variant</th>\n",
       "      <th>auc_train</th>\n",
       "      <th>auc_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.830461</td>\n",
       "      <td>0.758772</td>\n",
       "      <td>0.106951</td>\n",
       "      <td>0.785827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.828045</td>\n",
       "      <td>0.754404</td>\n",
       "      <td>0.106523</td>\n",
       "      <td>0.786149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.825912</td>\n",
       "      <td>0.749897</td>\n",
       "      <td>0.106004</td>\n",
       "      <td>0.786011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.825163</td>\n",
       "      <td>0.748335</td>\n",
       "      <td>0.104699</td>\n",
       "      <td>0.782502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.823859</td>\n",
       "      <td>0.755627</td>\n",
       "      <td>0.104243</td>\n",
       "      <td>0.781966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.823463</td>\n",
       "      <td>0.752925</td>\n",
       "      <td>0.104882</td>\n",
       "      <td>0.784234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.820402</td>\n",
       "      <td>0.763251</td>\n",
       "      <td>0.102013</td>\n",
       "      <td>0.777185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.820384</td>\n",
       "      <td>0.757615</td>\n",
       "      <td>0.102307</td>\n",
       "      <td>0.778166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>_scaled</td>\n",
       "      <td>0.820267</td>\n",
       "      <td>0.758091</td>\n",
       "      <td>0.102211</td>\n",
       "      <td>0.777936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>_woe_scaled</td>\n",
       "      <td>0.819057</td>\n",
       "      <td>0.755912</td>\n",
       "      <td>0.100935</td>\n",
       "      <td>0.774535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset data_variant  auc_train  auc_test  f1_train  accuracy_train\n",
       "44        3      _scaled   0.830461  0.758772  0.106951        0.785827\n",
       "53        3  _woe_scaled   0.828045  0.754404  0.106523        0.786149\n",
       "71        4  _woe_scaled   0.825912  0.749897  0.106004        0.786011\n",
       "26        2      _scaled   0.825163  0.748335  0.104699        0.782502\n",
       "35        2  _woe_scaled   0.823859  0.755627  0.104243        0.781966\n",
       "62        4      _scaled   0.823463  0.752925  0.104882        0.784234\n",
       "50        3  _woe_scaled   0.820402  0.763251  0.102013        0.777185\n",
       "8         1      _scaled   0.820384  0.757615  0.102307        0.778166\n",
       "41        3      _scaled   0.820267  0.758091  0.102211        0.777936\n",
       "32        2  _woe_scaled   0.819057  0.755912  0.100935        0.774535"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"xgboost_modelling.csv\", sep=';')\n",
    "results = results[results['auc_train'] - results['auc_test'] < 0.05]\n",
    "results = results.sort_values(by=['auc_train'], ascending=False)\n",
    "results = results[['dataset', 'data_variant', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train']]\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podsumowanie\n",
    "\n",
    "Sumarycznie przeprowadzono przeliczenia dla:\n",
    "\n",
    "- 4 różnych zestawów danych różniących się sposobem uzupełnienia wartości brakujacych,\n",
    "- różnych sposobów obróbki danych, z uwzględnieniem redukcji wymiarowości, skalowania, kodowania WOE dla zmiennych kategorycznych,\n",
    "- 5 różnych klasyfikatorów.\n",
    "\n",
    "Z uwagi na niepowodzenie w modelowaniu z wykorzystaniem Gradient Boostingu pominę wyniki tego klasyfikatora w zestawieniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['klasyfikator', 'auc_train', 'auc_test', 'f1_train', 'accuracy_train', 'TN_train', 'FN_train', 'FP_train', 'TP_train']\n",
    "columns_rename = {'auc_train':'AUC_train', 'auc_test':'AUC_test', 'f1_train':'F1', 'accuracy_train':'ACC', 'TN_train':'TN', 'FN_train':'FN', 'FP_train':'FP', 'TP_train':'TP'}\n",
    "columns_formats = {'AUC_train':'{:5.4f}', 'AUC_test':'{:5.4f}', 'F1':'{:5.4f}', 'ACC':'{:5.4f}', 'TN':'{:5.0f}', 'FN':'{:5.0f}','FP':'{:5.0f}', 'TP':'{:5.0f}' }\n",
    "\n",
    "logistic_regression = pd.read_csv(\"logistic_regression_modelling.csv\", sep = ';')\n",
    "decision_tree = pd.read_csv(\"decision_tree_modelling.csv\", sep = ';')\n",
    "random_forest = pd.read_csv(\"random_forest_modelling.csv\", sep = ';')\n",
    "xgboost = pd.read_csv(\"xgboost_modelling.csv\", sep = ';')\n",
    "\n",
    "logistic_regression['klasyfikator'] = 'regresja logistyczna' \n",
    "decision_tree['klasyfikator'] = 'drzewo decyzyjne' \n",
    "random_forest['klasyfikator'] = 'las losowy' \n",
    "xgboost['klasyfikator'] = 'XGBoost' \n",
    "\n",
    "logistic_regression = logistic_regression[columns].rename(columns_rename, axis=1)\n",
    "decision_tree = decision_tree[columns].rename(columns_rename, axis=1)\n",
    "random_forest = random_forest[columns].rename(columns_rename, axis=1)\n",
    "xgboost = xgboost[columns].rename(columns_rename, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej zestawiono miary dla najlepszych modeli uzyskanych w każdej z metod.\n",
    "\n",
    "a. najlepsze modele pod względem zwróconej miary AUC_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6a\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >klasyfikator</th> \n",
       "        <th class=\"col_heading level0 col1\" >AUC_train</th> \n",
       "        <th class=\"col_heading level0 col2\" >AUC_test</th> \n",
       "        <th class=\"col_heading level0 col3\" >F1</th> \n",
       "        <th class=\"col_heading level0 col4\" >ACC</th> \n",
       "        <th class=\"col_heading level0 col5\" >TN</th> \n",
       "        <th class=\"col_heading level0 col6\" >FN</th> \n",
       "        <th class=\"col_heading level0 col7\" >FP</th> \n",
       "        <th class=\"col_heading level0 col8\" >TP</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6alevel0_row0\" class=\"row_heading level0 row0\" >1143</th> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col0\" class=\"data row0 col0\" >las losowy</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col1\" class=\"data row0 col1\" >0.8432</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col2\" class=\"data row0 col2\" >0.7512</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col3\" class=\"data row0 col3\" >0.1160</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col4\" class=\"data row0 col4\" >0.8027</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col5\" class=\"data row0 col5\" >51545</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col6\" class=\"data row0 col6\" >12765</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col7\" class=\"data row0 col7\" >  110</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow0_col8\" class=\"data row0 col8\" >  845</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6alevel0_row1\" class=\"row_heading level0 row1\" >44</th> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col0\" class=\"data row1 col0\" >XGBoost</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col1\" class=\"data row1 col1\" >0.8305</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col2\" class=\"data row1 col2\" >0.7588</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col3\" class=\"data row1 col3\" >0.1070</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col4\" class=\"data row1 col4\" >0.7858</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col5\" class=\"data row1 col5\" >50450</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col6\" class=\"data row1 col6\" >13860</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col7\" class=\"data row1 col7\" >  118</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow1_col8\" class=\"data row1 col8\" >  837</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6alevel0_row2\" class=\"row_heading level0 row2\" >280</th> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col0\" class=\"data row2 col0\" >drzewo decyzyjne</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col1\" class=\"data row2 col1\" >0.8174</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col2\" class=\"data row2 col2\" >0.7105</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col3\" class=\"data row2 col3\" >0.0961</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col4\" class=\"data row2 col4\" >0.7581</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col5\" class=\"data row2 col5\" >48638</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col6\" class=\"data row2 col6\" >15672</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col7\" class=\"data row2 col7\" >  116</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow2_col8\" class=\"data row2 col8\" >  839</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6alevel0_row3\" class=\"row_heading level0 row3\" >133</th> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col0\" class=\"data row3 col0\" >regresja logistyczna</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col1\" class=\"data row3 col1\" >0.7602</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col2\" class=\"data row3 col2\" >0.7496</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col3\" class=\"data row3 col3\" >0.0738</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col4\" class=\"data row3 col4\" >0.6971</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col5\" class=\"data row3 col5\" >44711</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col6\" class=\"data row3 col6\" >19599</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col7\" class=\"data row3 col7\" >  167</td> \n",
       "        <td id=\"T_f8629640_b184_11e9_9f1d_408d5cb9ae6arow3_col8\" class=\"data row3 col8\" >  788</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18b45390>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_1 = logistic_regression.sort_values(by=['AUC_train'], ascending=False)[:1]\n",
    "decision_tree_1 = decision_tree.sort_values(by=['AUC_train'], ascending=False)[:1]\n",
    "random_forest_1 = random_forest.sort_values(by=['AUC_train'], ascending=False)[:1]\n",
    "xgboost_1 = xgboost.sort_values(by=['AUC_train'], ascending=False)[:1]\n",
    "best_AUC_train = pd.concat([logistic_regression_1, decision_tree_1, random_forest_1, xgboost_1])\n",
    "best_AUC_train = best_AUC_train.sort_values(by=['AUC_train'], ascending=False)\n",
    "best_AUC_train.style.format(columns_formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. najlepsze modele pod względem zwróconej miary AUC_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6a\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >klasyfikator</th> \n",
       "        <th class=\"col_heading level0 col1\" >AUC_train</th> \n",
       "        <th class=\"col_heading level0 col2\" >AUC_test</th> \n",
       "        <th class=\"col_heading level0 col3\" >F1</th> \n",
       "        <th class=\"col_heading level0 col4\" >ACC</th> \n",
       "        <th class=\"col_heading level0 col5\" >TN</th> \n",
       "        <th class=\"col_heading level0 col6\" >FN</th> \n",
       "        <th class=\"col_heading level0 col7\" >FP</th> \n",
       "        <th class=\"col_heading level0 col8\" >TP</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6alevel0_row0\" class=\"row_heading level0 row0\" >1263</th> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col0\" class=\"data row0 col0\" >las losowy</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col1\" class=\"data row0 col1\" >0.7908</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col2\" class=\"data row0 col2\" >0.7716</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col3\" class=\"data row0 col3\" >0.0880</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col4\" class=\"data row0 col4\" >0.7463</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col5\" class=\"data row0 col5\" >47910</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col6\" class=\"data row0 col6\" >16400</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col7\" class=\"data row0 col7\" >  156</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow0_col8\" class=\"data row0 col8\" >  799</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6alevel0_row1\" class=\"row_heading level0 row1\" >153</th> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col0\" class=\"data row1 col0\" >drzewo decyzyjne</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col1\" class=\"data row1 col1\" >0.7907</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col2\" class=\"data row1 col2\" >0.7700</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col3\" class=\"data row1 col3\" >0.0856</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col4\" class=\"data row1 col4\" >0.7349</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col5\" class=\"data row1 col5\" >47154</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col6\" class=\"data row1 col6\" >17156</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col7\" class=\"data row1 col7\" >  145</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow1_col8\" class=\"data row1 col8\" >  810</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6alevel0_row2\" class=\"row_heading level0 row2\" >13</th> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col0\" class=\"data row2 col0\" >XGBoost</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col1\" class=\"data row2 col1\" >0.7988</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col2\" class=\"data row2 col2\" >0.7673</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col3\" class=\"data row2 col3\" >0.0928</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col4\" class=\"data row2 col4\" >0.7600</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col5\" class=\"data row2 col5\" >48802</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col6\" class=\"data row2 col6\" >15508</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col7\" class=\"data row2 col7\" >  154</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow2_col8\" class=\"data row2 col8\" >  801</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6alevel0_row3\" class=\"row_heading level0 row3\" >70</th> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col0\" class=\"data row3 col0\" >regresja logistyczna</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col1\" class=\"data row3 col1\" >0.7551</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col2\" class=\"data row3 col2\" >0.7527</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col3\" class=\"data row3 col3\" >0.0718</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col4\" class=\"data row3 col4\" >0.6882</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col5\" class=\"data row3 col5\" >44126</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col6\" class=\"data row3 col6\" >20184</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col7\" class=\"data row3 col7\" >  168</td> \n",
       "        <td id=\"T_e259da58_b184_11e9_a9ce_408d5cb9ae6arow3_col8\" class=\"data row3 col8\" >  787</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xd824dd8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_2 = logistic_regression.sort_values(by=['AUC_test'], ascending=False)[:1]\n",
    "decision_tree_2 = decision_tree.sort_values(by=['AUC_test'], ascending=False)[:1]\n",
    "random_forest_2 = random_forest.sort_values(by=['AUC_test'], ascending=False)[:1]\n",
    "xgboost_2 = xgboost.sort_values(by=['AUC_test'], ascending=False)[:1]\n",
    "best_AUC_test = pd.concat([logistic_regression_2, decision_tree_2, random_forest_2, xgboost_2])\n",
    "best_AUC_test = best_AUC_test.sort_values(by=['AUC_test'], ascending=False)\n",
    "best_AUC_test.style.format(columns_formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. najlepsze modele pod względem zróconej miary F1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6a\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >klasyfikator</th> \n",
       "        <th class=\"col_heading level0 col1\" >AUC_train</th> \n",
       "        <th class=\"col_heading level0 col2\" >AUC_test</th> \n",
       "        <th class=\"col_heading level0 col3\" >F1</th> \n",
       "        <th class=\"col_heading level0 col4\" >ACC</th> \n",
       "        <th class=\"col_heading level0 col5\" >TN</th> \n",
       "        <th class=\"col_heading level0 col6\" >FN</th> \n",
       "        <th class=\"col_heading level0 col7\" >FP</th> \n",
       "        <th class=\"col_heading level0 col8\" >TP</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6alevel0_row0\" class=\"row_heading level0 row0\" >2096</th> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col0\" class=\"data row0 col0\" >las losowy</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col1\" class=\"data row0 col1\" >0.7226</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col2\" class=\"data row0 col2\" >0.5985</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col3\" class=\"data row0 col3\" >0.2612</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col4\" class=\"data row0 col4\" >0.9605</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col5\" class=\"data row0 col5\" >62230</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col6\" class=\"data row0 col6\" > 2080</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col7\" class=\"data row0 col7\" >  499</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow0_col8\" class=\"data row0 col8\" >  456</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6alevel0_row1\" class=\"row_heading level0 row1\" >44</th> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col0\" class=\"data row1 col0\" >XGBoost</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col1\" class=\"data row1 col1\" >0.8305</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col2\" class=\"data row1 col2\" >0.7588</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col3\" class=\"data row1 col3\" >0.1070</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col4\" class=\"data row1 col4\" >0.7858</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col5\" class=\"data row1 col5\" >50450</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col6\" class=\"data row1 col6\" >13860</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col7\" class=\"data row1 col7\" >  118</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow1_col8\" class=\"data row1 col8\" >  837</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6alevel0_row2\" class=\"row_heading level0 row2\" >377</th> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col0\" class=\"data row2 col0\" >drzewo decyzyjne</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col1\" class=\"data row2 col1\" >0.7969</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col2\" class=\"data row2 col2\" >0.6979</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col3\" class=\"data row2 col3\" >0.0992</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col4\" class=\"data row2 col4\" >0.7848</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col5\" class=\"data row2 col5\" >50446</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col6\" class=\"data row2 col6\" >13864</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col7\" class=\"data row2 col7\" >  182</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow2_col8\" class=\"data row2 col8\" >  773</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6alevel0_row3\" class=\"row_heading level0 row3\" >117</th> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col0\" class=\"data row3 col0\" >regresja logistyczna</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col1\" class=\"data row3 col1\" >0.7267</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col2\" class=\"data row3 col2\" >0.7007</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col3\" class=\"data row3 col3\" >0.0978</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col4\" class=\"data row3 col4\" >0.8334</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col5\" class=\"data row3 col5\" >53805</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col6\" class=\"data row3 col6\" >10505</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col7\" class=\"data row3 col7\" >  366</td> \n",
       "        <td id=\"T_05bb87c8_b185_11e9_882c_408d5cb9ae6arow3_col8\" class=\"data row3 col8\" >  589</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10ab13c8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_3 = logistic_regression.sort_values(by=['F1'], ascending=False)[:1]\n",
    "decision_tree_3 = decision_tree.sort_values(by=['F1'], ascending=False)[:1]\n",
    "random_forest_3 = random_forest.sort_values(by=['F1'], ascending=False)[:1]\n",
    "xgboost_3 = xgboost.sort_values(by=['F1'], ascending=False)[:1]\n",
    "best_F1 = pd.concat([logistic_regression_3, decision_tree_3, random_forest_3, xgboost_3])\n",
    "best_F1 = best_F1.sort_values(by=['F1'], ascending=False)\n",
    "best_F1.style.format(columns_formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzyskane modele mają słabe parametry jakościowe i raczej nie będą miały dobrej siły predykcyjnej.\n",
    "Najlepsze pod względem AUC_train są przetrenowane (za wyjątkiem najsłabszej regresji logistycznej).\n",
    "\n",
    "Faworyci pod względem AUC_test mają niskie wartości AUC na poziomie nie przekraczającym 80% przy bardzo niskim poziomie F1-score (poniżej 19%).\n",
    "\n",
    "Metodą, która zwróciła najlepszy wynik jest las losowy. \n",
    "Dla modelu maksymalizującego AUC_test - 79% AUC na zbiorze treningowym, 77% AUC na zbiorze testowym oraz prawie 9% F1-score.\n",
    "Wynik o podobnych parametrach zwróciła również metoda XGBoost.\n",
    "\n",
    "Trudno uzasadnić uzyskane wyniki bez dobrej znajomości zbioru danych.\n",
    "Możliwe, że parametry w datasecie nie są dobtymi predyktorami zmiennej objaśnianej.\n",
    "\n",
    "Prawdopodobnie zachowanie klientów determinującej wzięcie kredytu nie jest opisywane wyłącznie suchymi faktami takimi jak miejsce złożenia wniosku, wiek, zarobki, parametry oferty kredytowej. Zmienne te najwyraźniej mają niewielką siłę predykcyjną w tym zakresie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
